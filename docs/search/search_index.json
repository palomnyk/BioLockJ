{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BioLockJ optimizes your bioinformatics pipeline and metagenomics analysis. Modular design logically partitions analysis and expedites failure recovery Automated script generation eliminates syntax errors and ensures uniform execution Standardized OTU abundance tables facilitate analysis across datasets Batch scripts take advantage of parallelization on the cluster job queue configuration file consolidates project details into a principal reference document (and can reproduce analysis) BioModule interface provides a flexible mechanism for adding new functionality BioLockJ User Guide: Installation Getting Started Commands Pipeline Componenets the config file Properties Modules the metadata input files Dependencies Features Failure Recovery Validation Supported Environments Check dependencies before pipeline start Expand BioLockJ by Building Modules Examples and Templates Example Pipeline FAQ Citation Links for Developers See javadocs at https://ivoryc.github.io/BioLockJ/javadocs/ See the sheepdog_testing_suite at https://github.com/BioLockJ-Dev-Team/sheepdog_testing_suite for developtment tests. View this User Guide online: BioLockJ User Guide at https://ivoryc.github.io/BioLockJ/","title":"Home"},{"location":"#biolockj-user-guide","text":"Installation Getting Started Commands Pipeline Componenets the config file Properties Modules the metadata input files Dependencies Features Failure Recovery Validation Supported Environments Check dependencies before pipeline start Expand BioLockJ by Building Modules Examples and Templates Example Pipeline FAQ Citation","title":"BioLockJ User Guide:"},{"location":"#links-for-developers","text":"See javadocs at https://ivoryc.github.io/BioLockJ/javadocs/ See the sheepdog_testing_suite at https://github.com/BioLockJ-Dev-Team/sheepdog_testing_suite for developtment tests. View this User Guide online: BioLockJ User Guide at https://ivoryc.github.io/BioLockJ/","title":"Links for Developers"},{"location":"Building-Modules/","text":"Any Java class that implements the BioModule interface can be added to a BioLockJ pipeline. The BioLockJ v1.0 implementation is currently focused on metagenomics analysis, but the generalized application framework is not limited to this domain. Users can implement new BioModules to automate a wide variety of bioinformatics and report analytics. The BioModule interface was designed so that users can develop new modules on their own. Beginners See the BioModule hello world tutorial. Coding your module To create a new BioModule , simply extend one of the abstract Java superclasses, code it's abstract methods, and add it to your pipeline with #BioModule tag your Config file: 1. BioModuleImpl : Extend if a more specific interface does not apply 1. ScriptModuleImpl : Extend if your module generates and executes bash scripts 1. JavaModuleImpl : Extend if your module only runs Java code 1. ClassifierModuleImpl : Extend to support a new classifier program 1. ParserModuleImpl : Extend to parse output of a new classifier program 1. R_Module : Extend if your module generates and executes R scripts To support a new classifier, create 3 modules that implement the following interfaces: ClassifierModule : Implement to generate bash scripts needed to call classifier program ParserModule : Implement to parse classifier output, configure as classifier post-requisite OtuNode : Classifier specific implementation holds OTU information for 1 sequence BioModuleImpl is the top-level superclass for all modules. Method Description checkDependencies() Must override. Called before executeTask() to identify Configuration errors and perform runtime validations. executeTask() Must override. Executes core module logic. cleanUp() Called after executeTask() to run cleanup operations, update Config properties, etc. getInputFiles() Return previous module output. getModuleDir() Return module root directory. getOutputDir() Return module output directory. getPostRequisiteModules() Returns a list of BioModules to run after the current module. getPreRequisiteModules() Returns a list of BioModules to run before the current module. getSummary() Return output directory summary. Most modules override this method by adding module specific summary details to super.getSummary(). getTempDir() Return module temp directory. setModuleDir(path) Set module directory. ScriptModuleImpl extends BioModuleImpl : superclass for script-generating modules. Method Description buildScript(files) Must override. Called by executeTask() for datasets with forward reads only. The return type is a list of lists. Each nested list contains the bash script lines required to process 1 sample. Obtains sequence files from getInputFiles(). buildScriptForPairedReads(files) Calls back to buildScript(files) by default. Subclasses override this method to generate unique scripts for datasets containing paired reads. checkDependencies() Called before executeTask() to validate script.batchSize , script.exitOnError , script.numThreads , script.permissions , script.timeout getJobParams() Return shell command to execute the MAIN script. getScriptDir() Return module script directory. getSummary() Adds the script directory summary to super.getSummary(). Most modules override this method by adding module specific summary details to super.getSummary(). getTimeout() Return script.timeout . getWorkerScriptFunctions() Return bash script lines for any functions needed in the worker scripts. JavaModuleImpl extends ScriptModuleImpl : superclass for pure Java modules. To avoid running code on the cluster head node, a temporary instance of BioLockJ is spawned on a cluster node which is launched by the sole worker script from the job queue. Method Description runModule() Must override. Executes core module logic. buildScript(files) This method returns a single line calling java on the BioLockJ source code, passing -d parameter to run in direct mode and the full class name of the JavaModule to indicate the module to run. getSource() Determines if running code from Jar or source code in order to write valid bash script lines. getTimeout() Return java.timeout . moduleComplete() Create the script success indicator file. moduleFailed() Create the script failures indicator file. ClassifierModuleImpl extends ScriptModuleImpl : biolockj.module.classifier superclass. Method Description buildScriptForPairedReads(files) Called by executeTask() for datasets with paired reads. The return type is a list of lists, where each nested list contains the bash script lines required to process 1 sample. Obtains sequence files from SeqUtil .getPairedReads(getInputFiles()). checkDependencies() Validate Configuration properties exe.classifier and exe.classifierParams , verify sequence file format, log classifier version info, and verify no biolockj.module.seq modules are configured run after the ClassifierModule . Subclasses should call super.checkDependencies() if overriding this method to retain these verifications. executeTask() Call buildScript(files) or buildScriptForPairedReads(files) based input sequence format and calls BashScriptBuilder to generate the main script + 1 worker script for every script.batchSize samples. To change the batch scheme, override this method to call the alternate BashScriptBuilder .buildScripts() method signiture and hard code the batch size. All biolockj.module.classifier modules override this method. getClassifierExe() Return Configuration property exe.classifier to call the classifier program in the bash scripts. If the classifier is not included in cluster.modules , validate that value is a valid file path. If exe.classifier is undefined, replace the property prefix exe with the lowercase prefix of the module class name (less the standard module suffix classifier ). For example, use rdp.classifier for RdpClassifier and kraken.classifier for KrakenClassifier . This allows users to define all classifier programs in a default Configuration file rather than setting exe.clssifier in each project Configuration file. getClassifierParams() Return Configuration property exe.classifierParams which may contain a list of parameters (without hyphens) to pass to the classifier program in the bash scripts. If exe.classifierParams is undefined, replace the property prefix exe with the lowercase prefix of the module class name as described for exe.classifier . getSummary() Adds input directory summary to super.getSummary(). Most modules override this method to add module specific summary details to super.getSummary(). logVersion() Run exe.classifier --version to log version info. RDP overrides this method to return null since the version switch is not supported. ParserModuleImpl extends JavaModuleImpl : biolockj.module.implicit.parser superclass. Method Description parseSamples() Must override. Called by executeTask() to populate the Set returned by getParsedSamples(). Each classifier requires a unique parser module to decode its output. This method should iterate through the classifier reports to build OtuNode s for each sample-OTU found in the report. The OtuNode s are stored in a ParsedSample and cached via addParsedSample( ParsedSample ). addParsedSample( sample ) Add the ParsedSample to the Set returned by getParsedSamples(). buildOtuTables() Generate OTU abundance tables from ClassifierModule output. checkDependencies() Validate Configuration properties ( report.minOtuCount , report.minOtuThreshold , report.logBase ) and verify no biolockj.module.classifier modules are configured to run after the ParserModule . executeTask() If report.numHits =Y, add \"Num_Hits\" column to metadata containing the number of reads that map to any OTU for each sample. Calls buildOtuTables() to generate module output. getParsedSample(id) Return the ParsedSample from the the Set returned by getParsedSamples() for a given id. getParsedSamples() Return 1 ParsedSample for each classified sample in the dataset. OtuNodeImpl is the superclass for the biolockj.node package. Method Description addOtu(level, otu) A node represents a single OTU, each level in the taxonomic hierarchy is populated with this method. getCount() Get the OTU count. getLine() Get the classifier report line used to create the node. getOtuMap() This map may contain 1 element for each of the report.taxonomyLevels and is populated by addOtu(level, otu). getSampleId() Get the sample ID to which the OTU belongs. report() Print node info to log file as DEBUG line - not visible unless pipeline.logLevel=DEBUG . setCount(num) Set the OTU count. setLine(line) Set the classifier report line used to create the node. setSampleId(id) set the sample ID to which the OTU belongs. OtuNodeImpl methods do not need to be overridden. New OtuNode implementations should call existing methods from their constructor. Share your module Other users can use your module by putting the compiled class in the java classpath when they run BioLockj and using #BioModule classname in their config file. You can share your module as code or as a compiled class any way that you like. The official repository for external BioLockJ modules is blj_ext_modules . Each module has a folder at the top level of the repository and should include the java code as well a config file to test the module alone, a test file to run a multi-module pipeline that includes the module, and (where applicable) a dockerfile.","title":"Building Modules"},{"location":"Building-Modules/#any-java-class-that-implements-the-biomodule-interface-can-be-added-to-a-biolockj-pipeline","text":"The BioLockJ v1.0 implementation is currently focused on metagenomics analysis, but the generalized application framework is not limited to this domain. Users can implement new BioModules to automate a wide variety of bioinformatics and report analytics. The BioModule interface was designed so that users can develop new modules on their own.","title":"Any Java class that implements the BioModule interface can be added to a BioLockJ pipeline."},{"location":"Building-Modules/#beginners","text":"See the BioModule hello world tutorial.","title":"Beginners"},{"location":"Building-Modules/#coding-your-module","text":"To create a new BioModule , simply extend one of the abstract Java superclasses, code it's abstract methods, and add it to your pipeline with #BioModule tag your Config file: 1. BioModuleImpl : Extend if a more specific interface does not apply 1. ScriptModuleImpl : Extend if your module generates and executes bash scripts 1. JavaModuleImpl : Extend if your module only runs Java code 1. ClassifierModuleImpl : Extend to support a new classifier program 1. ParserModuleImpl : Extend to parse output of a new classifier program 1. R_Module : Extend if your module generates and executes R scripts","title":"Coding your module"},{"location":"Building-Modules/#to-support-a-new-classifier-create-3-modules-that-implement-the-following-interfaces","text":"ClassifierModule : Implement to generate bash scripts needed to call classifier program ParserModule : Implement to parse classifier output, configure as classifier post-requisite OtuNode : Classifier specific implementation holds OTU information for 1 sequence","title":"To support a new classifier, create 3 modules that implement the following interfaces:"},{"location":"Building-Modules/#biomoduleimpl-is-the-top-level-superclass-for-all-modules","text":"Method Description checkDependencies() Must override. Called before executeTask() to identify Configuration errors and perform runtime validations. executeTask() Must override. Executes core module logic. cleanUp() Called after executeTask() to run cleanup operations, update Config properties, etc. getInputFiles() Return previous module output. getModuleDir() Return module root directory. getOutputDir() Return module output directory. getPostRequisiteModules() Returns a list of BioModules to run after the current module. getPreRequisiteModules() Returns a list of BioModules to run before the current module. getSummary() Return output directory summary. Most modules override this method by adding module specific summary details to super.getSummary(). getTempDir() Return module temp directory. setModuleDir(path) Set module directory.","title":"BioModuleImpl is the top-level superclass for all modules."},{"location":"Building-Modules/#scriptmoduleimpl-extends-biomoduleimpl-superclass-for-script-generating-modules","text":"Method Description buildScript(files) Must override. Called by executeTask() for datasets with forward reads only. The return type is a list of lists. Each nested list contains the bash script lines required to process 1 sample. Obtains sequence files from getInputFiles(). buildScriptForPairedReads(files) Calls back to buildScript(files) by default. Subclasses override this method to generate unique scripts for datasets containing paired reads. checkDependencies() Called before executeTask() to validate script.batchSize , script.exitOnError , script.numThreads , script.permissions , script.timeout getJobParams() Return shell command to execute the MAIN script. getScriptDir() Return module script directory. getSummary() Adds the script directory summary to super.getSummary(). Most modules override this method by adding module specific summary details to super.getSummary(). getTimeout() Return script.timeout . getWorkerScriptFunctions() Return bash script lines for any functions needed in the worker scripts.","title":"ScriptModuleImpl extends BioModuleImpl:  superclass for script-generating modules."},{"location":"Building-Modules/#javamoduleimpl-extends-scriptmoduleimpl-superclass-for-pure-java-modules","text":"To avoid running code on the cluster head node, a temporary instance of BioLockJ is spawned on a cluster node which is launched by the sole worker script from the job queue. Method Description runModule() Must override. Executes core module logic. buildScript(files) This method returns a single line calling java on the BioLockJ source code, passing -d parameter to run in direct mode and the full class name of the JavaModule to indicate the module to run. getSource() Determines if running code from Jar or source code in order to write valid bash script lines. getTimeout() Return java.timeout . moduleComplete() Create the script success indicator file. moduleFailed() Create the script failures indicator file.","title":"JavaModuleImpl extends ScriptModuleImpl: superclass for pure Java modules."},{"location":"Building-Modules/#classifiermoduleimpl-extends-scriptmoduleimpl-biolockjmoduleclassifier-superclass","text":"Method Description buildScriptForPairedReads(files) Called by executeTask() for datasets with paired reads. The return type is a list of lists, where each nested list contains the bash script lines required to process 1 sample. Obtains sequence files from SeqUtil .getPairedReads(getInputFiles()). checkDependencies() Validate Configuration properties exe.classifier and exe.classifierParams , verify sequence file format, log classifier version info, and verify no biolockj.module.seq modules are configured run after the ClassifierModule . Subclasses should call super.checkDependencies() if overriding this method to retain these verifications. executeTask() Call buildScript(files) or buildScriptForPairedReads(files) based input sequence format and calls BashScriptBuilder to generate the main script + 1 worker script for every script.batchSize samples. To change the batch scheme, override this method to call the alternate BashScriptBuilder .buildScripts() method signiture and hard code the batch size. All biolockj.module.classifier modules override this method. getClassifierExe() Return Configuration property exe.classifier to call the classifier program in the bash scripts. If the classifier is not included in cluster.modules , validate that value is a valid file path. If exe.classifier is undefined, replace the property prefix exe with the lowercase prefix of the module class name (less the standard module suffix classifier ). For example, use rdp.classifier for RdpClassifier and kraken.classifier for KrakenClassifier . This allows users to define all classifier programs in a default Configuration file rather than setting exe.clssifier in each project Configuration file. getClassifierParams() Return Configuration property exe.classifierParams which may contain a list of parameters (without hyphens) to pass to the classifier program in the bash scripts. If exe.classifierParams is undefined, replace the property prefix exe with the lowercase prefix of the module class name as described for exe.classifier . getSummary() Adds input directory summary to super.getSummary(). Most modules override this method to add module specific summary details to super.getSummary(). logVersion() Run exe.classifier --version to log version info. RDP overrides this method to return null since the version switch is not supported.","title":"ClassifierModuleImpl extends ScriptModuleImpl: biolockj.module.classifier superclass."},{"location":"Building-Modules/#parsermoduleimpl-extends-javamoduleimpl-biolockjmoduleimplicitparser-superclass","text":"Method Description parseSamples() Must override. Called by executeTask() to populate the Set returned by getParsedSamples(). Each classifier requires a unique parser module to decode its output. This method should iterate through the classifier reports to build OtuNode s for each sample-OTU found in the report. The OtuNode s are stored in a ParsedSample and cached via addParsedSample( ParsedSample ). addParsedSample( sample ) Add the ParsedSample to the Set returned by getParsedSamples(). buildOtuTables() Generate OTU abundance tables from ClassifierModule output. checkDependencies() Validate Configuration properties ( report.minOtuCount , report.minOtuThreshold , report.logBase ) and verify no biolockj.module.classifier modules are configured to run after the ParserModule . executeTask() If report.numHits =Y, add \"Num_Hits\" column to metadata containing the number of reads that map to any OTU for each sample. Calls buildOtuTables() to generate module output. getParsedSample(id) Return the ParsedSample from the the Set returned by getParsedSamples() for a given id. getParsedSamples() Return 1 ParsedSample for each classified sample in the dataset.","title":"ParserModuleImpl extends JavaModuleImpl: biolockj.module.implicit.parser superclass."},{"location":"Building-Modules/#otunodeimpl-is-the-superclass-for-the-biolockjnode-package","text":"Method Description addOtu(level, otu) A node represents a single OTU, each level in the taxonomic hierarchy is populated with this method. getCount() Get the OTU count. getLine() Get the classifier report line used to create the node. getOtuMap() This map may contain 1 element for each of the report.taxonomyLevels and is populated by addOtu(level, otu). getSampleId() Get the sample ID to which the OTU belongs. report() Print node info to log file as DEBUG line - not visible unless pipeline.logLevel=DEBUG . setCount(num) Set the OTU count. setLine(line) Set the classifier report line used to create the node. setSampleId(id) set the sample ID to which the OTU belongs. OtuNodeImpl methods do not need to be overridden. New OtuNode implementations should call existing methods from their constructor.","title":"OtuNodeImpl is the superclass for the biolockj.node package."},{"location":"Building-Modules/#share-your-module","text":"Other users can use your module by putting the compiled class in the java classpath when they run BioLockj and using #BioModule classname in their config file. You can share your module as code or as a compiled class any way that you like. The official repository for external BioLockJ modules is blj_ext_modules . Each module has a folder at the top level of the repository and should include the java code as well a config file to test the module alone, a test file to run a multi-module pipeline that includes the module, and (where applicable) a dockerfile.","title":"Share your module"},{"location":"Built-in-modules/","text":"BioModules Some modules are packaged with BioLockJ (see below). To use modules created by a third-party, add the compiled files (jar file) to your biolockj extentions folder. When you call biolockj , use the --external-modules arg to pass in the location of the extra modules: biolockj --external-modules </path/to/extentions/folder> <config.properties> To create your own modules, see Building-Modules . In all cases, add modules to your BioModule order section to include them in your pipeline. Built-in BioModules: classifiers r16s classifiers wgs classifiers implicit modules implicit parsers module.implicit.parser.r16s.md module.implicit.parser.wgs.md implicit qiime modules report modules humann2 report by otu report by taxon R reports sequence modules DIY modules GenMod","title":"Modules"},{"location":"Built-in-modules/#biomodules","text":"Some modules are packaged with BioLockJ (see below). To use modules created by a third-party, add the compiled files (jar file) to your biolockj extentions folder. When you call biolockj , use the --external-modules arg to pass in the location of the extra modules: biolockj --external-modules </path/to/extentions/folder> <config.properties> To create your own modules, see Building-Modules . In all cases, add modules to your BioModule order section to include them in your pipeline.","title":"BioModules"},{"location":"Built-in-modules/#built-in-biomodules","text":"","title":"Built-in BioModules:"},{"location":"Built-in-modules/#classifiers","text":"r16s classifiers wgs classifiers","title":"classifiers"},{"location":"Built-in-modules/#implicit-modules","text":"implicit parsers module.implicit.parser.r16s.md module.implicit.parser.wgs.md implicit qiime modules","title":"implicit modules"},{"location":"Built-in-modules/#report-modules","text":"humann2 report by otu report by taxon R reports","title":"report modules"},{"location":"Built-in-modules/#sequence-modules","text":"","title":"sequence modules"},{"location":"Built-in-modules/#diy-modules","text":"GenMod","title":"DIY modules"},{"location":"Commands/","text":"BioLockJ commands are located under $BLJ/script Command Description biolockj [config_path] Start a pipeline using the Configuration properties in [config_path]. Pipeline output directory is created under $BLJ_PROJ . blj_go Go to most recent $BLJ_PROJ pipeline & list contents. blj_log Tail last 1K lines from current or most recent $BLJ_PROJ pipeline log file. blj_summary Print current or most recent $BLJ_PROJ pipeline summary. blj_complete Manually completes the current module and pipeline status. blj_reset Reset pipeline status to incomplete. If restarted, execution will start with the current module. blj_downlaod If on cluster, print command syntax to download current or most recent $BLJ_PROJ pipeline analysis to your local workstation directory: pipeline.downloadDir .","title":"Commands"},{"location":"Commands/#biolockj-commands-are-located-under-bljscript","text":"Command Description biolockj [config_path] Start a pipeline using the Configuration properties in [config_path]. Pipeline output directory is created under $BLJ_PROJ . blj_go Go to most recent $BLJ_PROJ pipeline & list contents. blj_log Tail last 1K lines from current or most recent $BLJ_PROJ pipeline log file. blj_summary Print current or most recent $BLJ_PROJ pipeline summary. blj_complete Manually completes the current module and pipeline status. blj_reset Reset pipeline status to incomplete. If restarted, execution will start with the current module. blj_downlaod If on cluster, print command syntax to download current or most recent $BLJ_PROJ pipeline analysis to your local workstation directory: pipeline.downloadDir .","title":"BioLockJ commands are located under $BLJ/script"},{"location":"Configuration/","text":"Configuration files contain all system properties, program inputs, cutoff values, external dependencies, and format specifications used during pipeline execution. BioLockJ takes a single configuration file as a runtime parameter. Although all properties can be configured in one file, we recommend chaining default files through the pipeline.defaultProps option. This can often improve the portability, maintainability, and readability of the project-specific configuration files. Standard Properties BioLockJ will always apply the standard.properties file packaged with BioLockJ under resources/config/default/ ; you do not need to specify this file in your pipeline.defaultProps chain. IFF running a pipeline in docker, then BioLockJ will apply the docker.properties file packaged with BioLockJ under resources/config/default/ . User-specified Defaults We recommend creating an environment.properties file to assign envionment-specific defaults. Set cluster & script properties Set paths to key executables through exe properties Override standard.properties as needed. This information is the same for many (or all) projects run in this environment, and entering the info anew for each project is tedious, time-consuming and error-prone. If using a shared system, consider using a user.properties file. Set user-specific properties such as download.dir and mail.to. For shared projects, use a path that will be updated per-user, such as ~/biolock_user.properties Other logical intermediates my also present themselves. For example, some group of projects may need to override several of the defaults set in environmment.properties, but others still use the those defaults. Projects in this set can use pipeline.defaultProps=group2.properties and the group2.properties files may include pipeline.defaultProps=environment.properties Project Properties Create a new configuration file for each pipeline to assign project-specific properties: Set the BioModule execution order Set pipeline.defaultProps = environment.properties You may use multiple default config files: pipeline.defaultProps=environment.properties,groupSettings.properties Override environment.properties and standard.properties as needed Example project configuration files can be found in templates . If the same property is given in multiple config files, the highest priority goes to the file used to launch the pipeline. Standard.properties always has the lowest priority. A copy of each configuration file is stored in the pipeline root directory to serve as primary project documentation. BioModule execution order To include a BioModule in your pipeline, add a #BioModule line to the top your configuration file, as shown in the examples found in templates . Each line has the #BioModule keyword followed by the path to the jar file for that module. For example: #BioModule biolockj.module.seq.PearMergeReads #BioModule biolockj.module.classifier.wgs.Kraken2Classifier #BioModule biolockj.module.report.r.R_PlotMds BioModules will be executed in the order they are listed in here. A typical pipeline contians one classifier module . Any number of sequence pre-processing modules may come before the classifier module. Any number of report modules may come after the classifier module. In addition to the BioModules specified in the configuration file, BioLockJ may add implicit modules that the are required by specified modules. See Example Pipeline . Summary of Properties Properties are defined as name-value pairs. List-values are comma separated. Leading and trailing whitespace is removed so \"propName=x,y\" is equivalent to \"propName = x, y\". aws Property Description aws.profile String aws.ram AWS memory applied through Nextflow. example value: \"8 GB\" aws.stack String aws.s3 String cluster Property Description cluster.batchCommand The command to submit jobs on the cluster cluster.host Cluster host address cluster.jobHeader Job script header to define # of nodes, # of cores, RAM, walltime, etc. cluster.modules List of modules to load before execution. Adds \u201cmodule load\u201d command to bash scripts cluster.prologue Command(s) to run at the start of every script after loading cluster modules (if any) cluster.runJavaAsScript Options: Y/N. If Y, each JavaModule will instantiate a clone of the application in direct mode on a job node via a single worker script to avoid overworking the head node where BioLockJ is deployed cluster.validateParams Options: Y/N. If Y, validate cluster.jobHeader \"ppn:\" or \"procs:\" value matches script.numThreads demultiplexer Property Description demultiplexer.barcodeCutoff desc demultimplexer.barcodeRevComp Options: Y/N. Use reverse compliment of metadata.barcodeColumn if demultimplexer.strategy = barcode_in_header or barcode_in_seq. demultimplexer.strategy Options: barcode_in_header, barcode_in_seq, id_in_header, do_not_demux. Set the Demultiplexer strategy. If using barcodes, they must be provided in the metadata.filePath with in column name defined by metadata.barcodeColumn . docker Property Description docker.imgVersion By default, docker will always use 'latest', but advanced users may specify a different tag. docker.user Docker Hub user name with the BioLockJ containers. By default the \"biolockj\" user is used to pull the standard modules, but advanced users can deploy their own versions of these modules and add new modules in their own Docker Hub account. docker.saveContainerOnExit Y/N. If Y, property removed the default --rm flag on docker run command exe Property Description exe.awk Define executable awk command, if default \"awk\" is not included in your $PATH exe.docker Define executable docker command, if default \"docker\" is not included in your $PATH exe.gzip Define executable gzip command, if default \"gzip\" is not included in your $PATH exe.humann2 Define executable humann2 command, if default \"humann2\" is not included in your $PATH exe.humann2Params Optional humann2 parameters exe.humann2JoinTableParams Optional parameters exe.humann2RenormTableParams Optional parameters exe.java Define executable java command, if default \"java\" is not included in your $PATH exe.javaParams Optional parameters exe.kneaddata Define executable kneaddata command, if default \"kneaddata\" is not included in your $PATH exe.kneaddataParams Optional kneaddata parameters exe.kraken Define executable kraken command, if default \"kraken\" is not included in your $PATH exe.krakenParams Optional kraken parameters exe.kraken2 Define executable kraken2 command, if default \"kraken2\" is not included in your $PATH exe.kraken2Params Optional kraken2 parameters exe.metaphlan2 Define executable metaphlan2 command, if default \"metaphlan2\" is not included in your $PATH exe.metaphlan2Params Optional metaphlan2 parameters exe.pear Define executable pear command, if default \"pear\" is not included in your $PATH exe.pearParams Optional pear parameters exe.python Define executable python command, if default \"python\" is not included in your $PATH exe.Rscript Define executable Rscript command, if default \"Rscript\" is not included in your $PATH exe.vsearch Define executable vsearch command, if default \"vsearch\" is not included in your $PATH exe.vsearchParams Optional vsearch parameters GenMod Property Description genMod.launcher Define executable language command if it is not included in your $PATH genMod.param Any parameters that is needed for user's script genMod.scriptPath Path where user script is stored humann2 Property Description humann2.disableGeneFamilies Options: Y/N. If Y, disable HumanN2 Gene Family report humann2.disablePathAbundance Options: Y/N. If Y, disable HumanN2 Pathway Abundance report humann2.disablePathCoverage Options: Y/N. If Y, disable HumanN2 Pathway Coverage report humann2.keepUnintegrated Options: Y/N. If Y, keep UNINTEGRATED column in count tables (otherwise this column is dropped) humann2.keepUnmapped Options: Y/N. If Y, keep UNMAPPED column in count tables (otherwise this column is dropped) humann2.nuclDB Directory property may contain multiple nucleotide database files humann2.protDB Directory property may contain protein nucleotide database files input Property Description input.dirPaths List of directories containing pipeline input files input.ignoreFiles List of files to ignore if found in * input.dirPaths* input.requireCompletePairs Options: Y/N. Stop pipeline if any unpaired FW or RV read sequence file is found input.suffixFw File name suffix to indicate a forward read input.suffixRv File name suffix to indicate a reverse read input.trimPrefix For files named by Sample ID, provide the prefix preceding the ID to trim when extracting Sample ID. For multiplexed sequences, provide any characters in the sequence header preceding the ID. For fastq, this value could be \u201c@\u201d if the sample ID was added to the header immediately after the \"@\" symbol. input.trimSuffix For files named by Sample ID, provide the suffix after the ID, often this is just the file extension. Do not include read direction indicators listed in input.suffixFw/input.suffixRv . For multiplexed sequences, provide 1st character in the sequence header found after every embedded Sample ID. If undefined, \u201c_\u201d is used as the default end-of-sample-ID delimiter. kneaddata Property Description kneaddata.dbs Path to database for KneadData program kraken Property Description kraken.db Path to kraken database kraken2 Property Description kraken2.db Path to kraken2 database mail Property Description mail.encryptedPassword Encrypted password from email.from account. If BioLockJ is passed a 2nd parameter (in addition to the config file), the 2nd parameter should be the clear-text password. The password will be encrypted and stored in the prop file for future use. WARNING: Base64 encryption is only a trivial roadblock for malicious users. This functionality is intended merely to keep clear-text passwords out of the configuration files and should only be used with a disposable email.from account. mail.from Notification emails sent from this account, provided email.encryptedPassword is valid mail.smtp.auth Options: Y/N. Set the SMTP authorization property mail.smtp.host Email SMTP Host mail.smtp.port Email SMTP Host mail.smtp.starttls.enable Options: Y/N. Set the SMTP start TLS property mail.to Comma-separated email recipients list metadata Property Description metadata.barcodeColumn Metadata column name containing the barcode used for demultiplexing metadata.columnDelim Define column delimiter for metadata.filePath file, default = tab metadata.commentChar Define how comments are indicated in metadata.filePath file, default = \"\" metadata.fileNameColumn Column in metadata file giving file names used to identify each sample. Standard default: \"InputFileName\". Values should be simple names, not file paths, and unique to each sample. Using this column in the metadata overrides the use of input.trimPreifx and input.trimSuffix. For paired reads, give the forward read file and use input.suffixFw and input.suffixRv to link to the reverse file. metadata.filePath Metadata file path, must have unique column headers metadata.nullValue Define how null values are represented in metadata metadata.required Options: Y/N. Require every sequence file has a corresponding row in metadata file metadata.useEveryRow Options: Y/N. Requires every metadata row to have a corresponding sequence file metaphlan2 Property Description metaphlan2.db Directory property containing alternate database. Must always be paired with metaphlan2.mpa_pkl metaphlan2.mpa_pkl File property containing path to the mpa_pkl file used to reference an alternate DB. Must always be paired with metaphlan2.db multiplexer Property Description multiplexer.gzip Options: Y/N. If Y, gzip the multiplexed output pipeline Property Description pipeline.copyInput Options: Y/N. If Y, copy input.dirPaths into a new directory under the project root directory pipeline.defaultDemultiplexer Assign module to demultiplex datasets. Default = Demultiplexer pipeline.defaultFastaConverter Assign module to convert fastq sequence files into fasta format when required. Default = AwkFastaConverter pipeline.defaultSeqMerger Assign module to merge paired reads when required. Default = PearMergeReads pipeline.defaultStatsModule Java class name for default module used generate p-value and other stats pipeline.defaultProps Path to a default BioLockJ configuration file containing default property values that are overridden if defined in the primary configuration file pipeline.deleteTempFiles Options: Y/N. If Y, delete module temp dirs after execution pipeline.disableAddImplicitModules Options: Y/N. If Y, implicit modules will not be added to the pipeline pipeline.disableAddPreReqModules Options: Y/N. If Y, prerequisite modules will not be added to the pipeline. pipeline.downloadDir The pipeline summary includes an scp command for the user to download the pipeline analysis if executed on a cluster server. This property defines the target directory on the users workstation to which the analysis will be downloaded. pipeline.env Options: aws, cluster, local. Describes runtime environment pipeline.limitDebugClasses used to limit classes that log debug statements pipeline.logLevel Options: DEBUG, INFO, WARN, ERROR. Determines Java log level sensitivity pipeline.permissions Set chmod -R command security bits on pipeline root directory (Ex. 770) pipeline.userProfile Bash users typically use ~/.bash_profile (the standard default). qiime Property Description qiime.alphaMetrics Options listed online: scikit-bio.org qiime.params Optional parameters passed to qiime scripts qiime.pynastAlignDB File property to define ~/.qiime_config pynast_template_alignment_fp. If supplied, qiime.refSeqDB and qiime.taxaDB must also be supplied and all three must share some parent directory. qiime.refSeqDB File property to define ~/.qiime_config pick_otus_reference_seqs_fp and assign_taxonomy_reference_seqs_fp. If supplied, qiime.pynastAlignDB and qiime.taxaDB must also be supplied and all three must share some parent directory. qiime.removeChimeras Options: Y/N. If Y, remove chimeras after open or de novo OTU picking using exe.vsearch qiime.taxaDB File property to define ~/.qiime_config assign_taxonomy_id_to_taxonomy_fp. If supplied, qiime.pynastAlignDB and qiime.refSeqDB must also be supplied and all three must share some parent directory. r Property Description r.colorBase This is the base color used for labels & headings in the PDF report r.colorHighlight This color is used to highlight significant OTU plot titles r.colorPalette palette argument passed to get_palette {ggpubr} to select colors for some output visualiztions r.colorPoint Sets the color of scatterplot and strip-chart plot points r.debug Options: Y/N. If Y, will generate R Script log files r.excludeFields List metadata columns to exclude from R script reports r.nominalFields Explicitly override default field type assignment to model as a nominal field in R r.numericFields Explicitly override default field type assignment to model as a numeric field in R r.pch Sets R plot pch parameter for PDF report r.pvalCutoff Sets p-value cutoff used to assign label r.colorHighlight r.pValFormat Sets the format used in R sprintf() function r.rareOtuThreshold If >1, R will filter OTUs below value provided. If <1, R will interperate the value as a percentage and discard OTUs not found in at least that percentage of samples r.reportFields Override field used to explicitly list metadata columns to report in the R scripts. If left undefined, all columns are reported r.saveRData Options: Y/N. If Y, all R script generating BioModules will save R Session data to the module output directory to a file using the extension \".RData\" r.timeout Sets # minutes before R Script will time out and fail r_CalculateStats Property Description r_CalculateStats.pAdjustScope Options: GLOBAL, LOCAL, TAXA, ATTRIBUTE. Used to set the p.adjust \"n\" parameter for how many simultaneous p-value calculations r_CalculateStats.pAdjustMethod Sets the p.adjust \"method\" parameter r_PlotEffectSize Property Description r_PlotEffectSize.parametricPval Options: Y/N. If Y, the parametric p-value is used when determining which taxa to include in the plot and which should get a (*). If N (default), the non-parametric p-value is used. r_PlotEffectSize.disablePvalAdj Options: Y/N. If Y, the non-adjusted p-value is used when determining which taxa to include in the plot and which should get a (*). If N (default), the adjusted p-value is used. r_PlotEffectSize.excludePvalAbove Options: [0,1], Taxa with a p-value above this value are excluded from the plot. r_PlotEffectSize.taxa Override other criteria for selecting which taxa to include in the plot by specifying wich taxa should be included r_PlotEffectSize.maxNumTaxa Each plot is given one page. This is the maximum number of bars to include in each one-page plot. r_PlotEffectSize.disableCohensD Options: Y/N. If N (default), produce plots for binary attributes showing effect size calculated as Cohen's d. If Y, skip this plot type. r_PlotEffectSize.disableRSquared Options: Y/N. If N (default), produce plots showing effect size calculated as the r-squared value. If Y, skip this plot type. r_PlotEffectSize.disableFoldChange Options: Y/N. If N (default), produce plots for binary attributes showing the fold change. If Y, skip this plot type. r_PlotMds Property Description r_PlotMds.numAxis Sets # MDS axis to plot r_PlotMds.distance distance metric for calculating MDS (default: bray) r_PlotMds.reportFields Override field used to explicitly list metadata columns to build MDS plots. If left undefined, all columns are reported rarefyOtuCounts Property Description rarefyOtuCounts.iterations Positive integer. The number of iterations to randomly select the rarefyOtuCounts.quantile of OTUs rarefyOtuCounts.lowAbundantCutoff Minimum percentage of samples that must contain an OTU. rarefyOtuCounts.quantile Quantile for rarefication. The number of OTUs/sample are ordered, all samples with more OTUs than the quantile sample are subselected without replacement until they have the same number of OTUs as the quantile sample rarefyOtuCounts.rmLowSamples Options: Y/N. If Y, all samples below the rarefyOtuCounts.quantile quantile sample are removed rarefySeqs Property Description rarefySeqs.max Randomly select maximum number of sequences per sample rarefySeqs.min Discard samples without minimum number of sequences rdp Property Description rdp.db File property used to define an alternate RDP database file rdp.jar File property for RDP java executable JAR rdp.minThresholdScore Required RDP minimum threshold score for valid OTUs report Property Description report.logBase Options: 10/e. If e, use natural log (base e), otherwise use log base 10 report.minCount Integer, minimum table count allowed. If a count less that this value is found, it is set to 0. report.numHits Options: Y/N. If Y, and add Num_Hits to metadata report.numReads Options: Y/N. If Y, and add Num_Reads to metadata report.scarceCountCutoff Minimum percentage of samples that must contain a count value for it to be kept. report.scarceSampleCutoff Minimum percentage of data columns that must be non-zero to keep the sample. report.taxonomyLevels Options: domain, phylum, class, order, family, genus, species. Generate reports for listed taxonomy levels script Property Description script.batchSize Number of sequence files to process per worker script script.defaultHeader Used to set shebang line to define scripts as bash executables, such as \"#!/bin/bash\" script.numThreads Integer value passed to any module that takes a number of threads parameter script.permissions Set chmod command security bits on generated scripts (Ex. 770) script.timeout Integer, time (minutes) before worker scripts times out. seqFileValidator Property Description seqFileValidator.requireEqualNumPairs Options: Y/N. default Y. seqFileValidator.seqMaxLen maximum number of bases per read seqFileValidator.seqMinLen minimum number of bases per read trimPrimers Property Description trimPrimers.filePath Path to file containing one primer sequence per line. trimPrimers.requirePrimer Options: Y/N. If Y, TrimPrimers will discard reads that do not include a primer sequence. validation Property Description validation.compareOn Which columns in the expectation file should be used for the comparison. Options: name, size, md5. Default: use all columns in the expectation file. validation.disableValidation Turn off validation. No validation file output is produced. Options: Y/N. default: N validation.expectationFile File path to the table of expectations. If a directory is given, BioLockJ will look for a file named after the module being evaluated. validation.reportOn Which attributes of the file should be included in the validation report file. Options: name, size, md5 validation.sizeWithinPercent What percentage difference is permitted between an output file and its expectation. Options: any positive number validation.stopPipeline If enabled, the validation utlility will stop the pipeline if any module fails validation. Options: Y/N","title":"Configuration"},{"location":"Configuration/#standard-properties","text":"BioLockJ will always apply the standard.properties file packaged with BioLockJ under resources/config/default/ ; you do not need to specify this file in your pipeline.defaultProps chain. IFF running a pipeline in docker, then BioLockJ will apply the docker.properties file packaged with BioLockJ under resources/config/default/ .","title":"Standard Properties"},{"location":"Configuration/#user-specified-defaults","text":"We recommend creating an environment.properties file to assign envionment-specific defaults. Set cluster & script properties Set paths to key executables through exe properties Override standard.properties as needed. This information is the same for many (or all) projects run in this environment, and entering the info anew for each project is tedious, time-consuming and error-prone. If using a shared system, consider using a user.properties file. Set user-specific properties such as download.dir and mail.to. For shared projects, use a path that will be updated per-user, such as ~/biolock_user.properties Other logical intermediates my also present themselves. For example, some group of projects may need to override several of the defaults set in environmment.properties, but others still use the those defaults. Projects in this set can use pipeline.defaultProps=group2.properties and the group2.properties files may include pipeline.defaultProps=environment.properties","title":"User-specified Defaults"},{"location":"Configuration/#project-properties","text":"Create a new configuration file for each pipeline to assign project-specific properties: Set the BioModule execution order Set pipeline.defaultProps = environment.properties You may use multiple default config files: pipeline.defaultProps=environment.properties,groupSettings.properties Override environment.properties and standard.properties as needed Example project configuration files can be found in templates . If the same property is given in multiple config files, the highest priority goes to the file used to launch the pipeline. Standard.properties always has the lowest priority. A copy of each configuration file is stored in the pipeline root directory to serve as primary project documentation.","title":"Project Properties"},{"location":"Configuration/#biomodule-execution-order","text":"To include a BioModule in your pipeline, add a #BioModule line to the top your configuration file, as shown in the examples found in templates . Each line has the #BioModule keyword followed by the path to the jar file for that module. For example: #BioModule biolockj.module.seq.PearMergeReads #BioModule biolockj.module.classifier.wgs.Kraken2Classifier #BioModule biolockj.module.report.r.R_PlotMds BioModules will be executed in the order they are listed in here. A typical pipeline contians one classifier module . Any number of sequence pre-processing modules may come before the classifier module. Any number of report modules may come after the classifier module. In addition to the BioModules specified in the configuration file, BioLockJ may add implicit modules that the are required by specified modules. See Example Pipeline .","title":"BioModule execution order"},{"location":"Configuration/#summary-of-properties","text":"Properties are defined as name-value pairs. List-values are comma separated. Leading and trailing whitespace is removed so \"propName=x,y\" is equivalent to \"propName = x, y\".","title":"Summary of Properties"},{"location":"Configuration/#aws","text":"Property Description aws.profile String aws.ram AWS memory applied through Nextflow. example value: \"8 GB\" aws.stack String aws.s3 String","title":"aws"},{"location":"Configuration/#cluster","text":"Property Description cluster.batchCommand The command to submit jobs on the cluster cluster.host Cluster host address cluster.jobHeader Job script header to define # of nodes, # of cores, RAM, walltime, etc. cluster.modules List of modules to load before execution. Adds \u201cmodule load\u201d command to bash scripts cluster.prologue Command(s) to run at the start of every script after loading cluster modules (if any) cluster.runJavaAsScript Options: Y/N. If Y, each JavaModule will instantiate a clone of the application in direct mode on a job node via a single worker script to avoid overworking the head node where BioLockJ is deployed cluster.validateParams Options: Y/N. If Y, validate cluster.jobHeader \"ppn:\" or \"procs:\" value matches script.numThreads","title":"cluster"},{"location":"Configuration/#demultiplexer","text":"Property Description demultiplexer.barcodeCutoff desc demultimplexer.barcodeRevComp Options: Y/N. Use reverse compliment of metadata.barcodeColumn if demultimplexer.strategy = barcode_in_header or barcode_in_seq. demultimplexer.strategy Options: barcode_in_header, barcode_in_seq, id_in_header, do_not_demux. Set the Demultiplexer strategy. If using barcodes, they must be provided in the metadata.filePath with in column name defined by metadata.barcodeColumn .","title":"demultiplexer"},{"location":"Configuration/#docker","text":"Property Description docker.imgVersion By default, docker will always use 'latest', but advanced users may specify a different tag. docker.user Docker Hub user name with the BioLockJ containers. By default the \"biolockj\" user is used to pull the standard modules, but advanced users can deploy their own versions of these modules and add new modules in their own Docker Hub account. docker.saveContainerOnExit Y/N. If Y, property removed the default --rm flag on docker run command","title":"docker"},{"location":"Configuration/#exe","text":"Property Description exe.awk Define executable awk command, if default \"awk\" is not included in your $PATH exe.docker Define executable docker command, if default \"docker\" is not included in your $PATH exe.gzip Define executable gzip command, if default \"gzip\" is not included in your $PATH exe.humann2 Define executable humann2 command, if default \"humann2\" is not included in your $PATH exe.humann2Params Optional humann2 parameters exe.humann2JoinTableParams Optional parameters exe.humann2RenormTableParams Optional parameters exe.java Define executable java command, if default \"java\" is not included in your $PATH exe.javaParams Optional parameters exe.kneaddata Define executable kneaddata command, if default \"kneaddata\" is not included in your $PATH exe.kneaddataParams Optional kneaddata parameters exe.kraken Define executable kraken command, if default \"kraken\" is not included in your $PATH exe.krakenParams Optional kraken parameters exe.kraken2 Define executable kraken2 command, if default \"kraken2\" is not included in your $PATH exe.kraken2Params Optional kraken2 parameters exe.metaphlan2 Define executable metaphlan2 command, if default \"metaphlan2\" is not included in your $PATH exe.metaphlan2Params Optional metaphlan2 parameters exe.pear Define executable pear command, if default \"pear\" is not included in your $PATH exe.pearParams Optional pear parameters exe.python Define executable python command, if default \"python\" is not included in your $PATH exe.Rscript Define executable Rscript command, if default \"Rscript\" is not included in your $PATH exe.vsearch Define executable vsearch command, if default \"vsearch\" is not included in your $PATH exe.vsearchParams Optional vsearch parameters","title":"exe"},{"location":"Configuration/#genmod","text":"Property Description genMod.launcher Define executable language command if it is not included in your $PATH genMod.param Any parameters that is needed for user's script genMod.scriptPath Path where user script is stored","title":"GenMod"},{"location":"Configuration/#humann2","text":"Property Description humann2.disableGeneFamilies Options: Y/N. If Y, disable HumanN2 Gene Family report humann2.disablePathAbundance Options: Y/N. If Y, disable HumanN2 Pathway Abundance report humann2.disablePathCoverage Options: Y/N. If Y, disable HumanN2 Pathway Coverage report humann2.keepUnintegrated Options: Y/N. If Y, keep UNINTEGRATED column in count tables (otherwise this column is dropped) humann2.keepUnmapped Options: Y/N. If Y, keep UNMAPPED column in count tables (otherwise this column is dropped) humann2.nuclDB Directory property may contain multiple nucleotide database files humann2.protDB Directory property may contain protein nucleotide database files","title":"humann2"},{"location":"Configuration/#input","text":"Property Description input.dirPaths List of directories containing pipeline input files input.ignoreFiles List of files to ignore if found in * input.dirPaths* input.requireCompletePairs Options: Y/N. Stop pipeline if any unpaired FW or RV read sequence file is found input.suffixFw File name suffix to indicate a forward read input.suffixRv File name suffix to indicate a reverse read input.trimPrefix For files named by Sample ID, provide the prefix preceding the ID to trim when extracting Sample ID. For multiplexed sequences, provide any characters in the sequence header preceding the ID. For fastq, this value could be \u201c@\u201d if the sample ID was added to the header immediately after the \"@\" symbol. input.trimSuffix For files named by Sample ID, provide the suffix after the ID, often this is just the file extension. Do not include read direction indicators listed in input.suffixFw/input.suffixRv . For multiplexed sequences, provide 1st character in the sequence header found after every embedded Sample ID. If undefined, \u201c_\u201d is used as the default end-of-sample-ID delimiter.","title":"input"},{"location":"Configuration/#kneaddata","text":"Property Description kneaddata.dbs Path to database for KneadData program","title":"kneaddata"},{"location":"Configuration/#kraken","text":"Property Description kraken.db Path to kraken database","title":"kraken"},{"location":"Configuration/#kraken2","text":"Property Description kraken2.db Path to kraken2 database","title":"kraken2"},{"location":"Configuration/#mail","text":"Property Description mail.encryptedPassword Encrypted password from email.from account. If BioLockJ is passed a 2nd parameter (in addition to the config file), the 2nd parameter should be the clear-text password. The password will be encrypted and stored in the prop file for future use. WARNING: Base64 encryption is only a trivial roadblock for malicious users. This functionality is intended merely to keep clear-text passwords out of the configuration files and should only be used with a disposable email.from account. mail.from Notification emails sent from this account, provided email.encryptedPassword is valid mail.smtp.auth Options: Y/N. Set the SMTP authorization property mail.smtp.host Email SMTP Host mail.smtp.port Email SMTP Host mail.smtp.starttls.enable Options: Y/N. Set the SMTP start TLS property mail.to Comma-separated email recipients list","title":"mail"},{"location":"Configuration/#metadata","text":"Property Description metadata.barcodeColumn Metadata column name containing the barcode used for demultiplexing metadata.columnDelim Define column delimiter for metadata.filePath file, default = tab metadata.commentChar Define how comments are indicated in metadata.filePath file, default = \"\" metadata.fileNameColumn Column in metadata file giving file names used to identify each sample. Standard default: \"InputFileName\". Values should be simple names, not file paths, and unique to each sample. Using this column in the metadata overrides the use of input.trimPreifx and input.trimSuffix. For paired reads, give the forward read file and use input.suffixFw and input.suffixRv to link to the reverse file. metadata.filePath Metadata file path, must have unique column headers metadata.nullValue Define how null values are represented in metadata metadata.required Options: Y/N. Require every sequence file has a corresponding row in metadata file metadata.useEveryRow Options: Y/N. Requires every metadata row to have a corresponding sequence file","title":"metadata"},{"location":"Configuration/#metaphlan2","text":"Property Description metaphlan2.db Directory property containing alternate database. Must always be paired with metaphlan2.mpa_pkl metaphlan2.mpa_pkl File property containing path to the mpa_pkl file used to reference an alternate DB. Must always be paired with metaphlan2.db","title":"metaphlan2"},{"location":"Configuration/#multiplexer","text":"Property Description multiplexer.gzip Options: Y/N. If Y, gzip the multiplexed output","title":"multiplexer"},{"location":"Configuration/#pipeline","text":"Property Description pipeline.copyInput Options: Y/N. If Y, copy input.dirPaths into a new directory under the project root directory pipeline.defaultDemultiplexer Assign module to demultiplex datasets. Default = Demultiplexer pipeline.defaultFastaConverter Assign module to convert fastq sequence files into fasta format when required. Default = AwkFastaConverter pipeline.defaultSeqMerger Assign module to merge paired reads when required. Default = PearMergeReads pipeline.defaultStatsModule Java class name for default module used generate p-value and other stats pipeline.defaultProps Path to a default BioLockJ configuration file containing default property values that are overridden if defined in the primary configuration file pipeline.deleteTempFiles Options: Y/N. If Y, delete module temp dirs after execution pipeline.disableAddImplicitModules Options: Y/N. If Y, implicit modules will not be added to the pipeline pipeline.disableAddPreReqModules Options: Y/N. If Y, prerequisite modules will not be added to the pipeline. pipeline.downloadDir The pipeline summary includes an scp command for the user to download the pipeline analysis if executed on a cluster server. This property defines the target directory on the users workstation to which the analysis will be downloaded. pipeline.env Options: aws, cluster, local. Describes runtime environment pipeline.limitDebugClasses used to limit classes that log debug statements pipeline.logLevel Options: DEBUG, INFO, WARN, ERROR. Determines Java log level sensitivity pipeline.permissions Set chmod -R command security bits on pipeline root directory (Ex. 770) pipeline.userProfile Bash users typically use ~/.bash_profile (the standard default).","title":"pipeline"},{"location":"Configuration/#qiime","text":"Property Description qiime.alphaMetrics Options listed online: scikit-bio.org qiime.params Optional parameters passed to qiime scripts qiime.pynastAlignDB File property to define ~/.qiime_config pynast_template_alignment_fp. If supplied, qiime.refSeqDB and qiime.taxaDB must also be supplied and all three must share some parent directory. qiime.refSeqDB File property to define ~/.qiime_config pick_otus_reference_seqs_fp and assign_taxonomy_reference_seqs_fp. If supplied, qiime.pynastAlignDB and qiime.taxaDB must also be supplied and all three must share some parent directory. qiime.removeChimeras Options: Y/N. If Y, remove chimeras after open or de novo OTU picking using exe.vsearch qiime.taxaDB File property to define ~/.qiime_config assign_taxonomy_id_to_taxonomy_fp. If supplied, qiime.pynastAlignDB and qiime.refSeqDB must also be supplied and all three must share some parent directory.","title":"qiime"},{"location":"Configuration/#r","text":"Property Description r.colorBase This is the base color used for labels & headings in the PDF report r.colorHighlight This color is used to highlight significant OTU plot titles r.colorPalette palette argument passed to get_palette {ggpubr} to select colors for some output visualiztions r.colorPoint Sets the color of scatterplot and strip-chart plot points r.debug Options: Y/N. If Y, will generate R Script log files r.excludeFields List metadata columns to exclude from R script reports r.nominalFields Explicitly override default field type assignment to model as a nominal field in R r.numericFields Explicitly override default field type assignment to model as a numeric field in R r.pch Sets R plot pch parameter for PDF report r.pvalCutoff Sets p-value cutoff used to assign label r.colorHighlight r.pValFormat Sets the format used in R sprintf() function r.rareOtuThreshold If >1, R will filter OTUs below value provided. If <1, R will interperate the value as a percentage and discard OTUs not found in at least that percentage of samples r.reportFields Override field used to explicitly list metadata columns to report in the R scripts. If left undefined, all columns are reported r.saveRData Options: Y/N. If Y, all R script generating BioModules will save R Session data to the module output directory to a file using the extension \".RData\" r.timeout Sets # minutes before R Script will time out and fail","title":"r"},{"location":"Configuration/#r_calculatestats","text":"Property Description r_CalculateStats.pAdjustScope Options: GLOBAL, LOCAL, TAXA, ATTRIBUTE. Used to set the p.adjust \"n\" parameter for how many simultaneous p-value calculations r_CalculateStats.pAdjustMethod Sets the p.adjust \"method\" parameter","title":"r_CalculateStats"},{"location":"Configuration/#r_ploteffectsize","text":"Property Description r_PlotEffectSize.parametricPval Options: Y/N. If Y, the parametric p-value is used when determining which taxa to include in the plot and which should get a (*). If N (default), the non-parametric p-value is used. r_PlotEffectSize.disablePvalAdj Options: Y/N. If Y, the non-adjusted p-value is used when determining which taxa to include in the plot and which should get a (*). If N (default), the adjusted p-value is used. r_PlotEffectSize.excludePvalAbove Options: [0,1], Taxa with a p-value above this value are excluded from the plot. r_PlotEffectSize.taxa Override other criteria for selecting which taxa to include in the plot by specifying wich taxa should be included r_PlotEffectSize.maxNumTaxa Each plot is given one page. This is the maximum number of bars to include in each one-page plot. r_PlotEffectSize.disableCohensD Options: Y/N. If N (default), produce plots for binary attributes showing effect size calculated as Cohen's d. If Y, skip this plot type. r_PlotEffectSize.disableRSquared Options: Y/N. If N (default), produce plots showing effect size calculated as the r-squared value. If Y, skip this plot type. r_PlotEffectSize.disableFoldChange Options: Y/N. If N (default), produce plots for binary attributes showing the fold change. If Y, skip this plot type.","title":"r_PlotEffectSize"},{"location":"Configuration/#r_plotmds","text":"Property Description r_PlotMds.numAxis Sets # MDS axis to plot r_PlotMds.distance distance metric for calculating MDS (default: bray) r_PlotMds.reportFields Override field used to explicitly list metadata columns to build MDS plots. If left undefined, all columns are reported","title":"r_PlotMds"},{"location":"Configuration/#rarefyotucounts","text":"Property Description rarefyOtuCounts.iterations Positive integer. The number of iterations to randomly select the rarefyOtuCounts.quantile of OTUs rarefyOtuCounts.lowAbundantCutoff Minimum percentage of samples that must contain an OTU. rarefyOtuCounts.quantile Quantile for rarefication. The number of OTUs/sample are ordered, all samples with more OTUs than the quantile sample are subselected without replacement until they have the same number of OTUs as the quantile sample rarefyOtuCounts.rmLowSamples Options: Y/N. If Y, all samples below the rarefyOtuCounts.quantile quantile sample are removed","title":"rarefyOtuCounts"},{"location":"Configuration/#rarefyseqs","text":"Property Description rarefySeqs.max Randomly select maximum number of sequences per sample rarefySeqs.min Discard samples without minimum number of sequences","title":"rarefySeqs"},{"location":"Configuration/#rdp","text":"Property Description rdp.db File property used to define an alternate RDP database file rdp.jar File property for RDP java executable JAR rdp.minThresholdScore Required RDP minimum threshold score for valid OTUs","title":"rdp"},{"location":"Configuration/#report","text":"Property Description report.logBase Options: 10/e. If e, use natural log (base e), otherwise use log base 10 report.minCount Integer, minimum table count allowed. If a count less that this value is found, it is set to 0. report.numHits Options: Y/N. If Y, and add Num_Hits to metadata report.numReads Options: Y/N. If Y, and add Num_Reads to metadata report.scarceCountCutoff Minimum percentage of samples that must contain a count value for it to be kept. report.scarceSampleCutoff Minimum percentage of data columns that must be non-zero to keep the sample. report.taxonomyLevels Options: domain, phylum, class, order, family, genus, species. Generate reports for listed taxonomy levels","title":"report"},{"location":"Configuration/#script","text":"Property Description script.batchSize Number of sequence files to process per worker script script.defaultHeader Used to set shebang line to define scripts as bash executables, such as \"#!/bin/bash\" script.numThreads Integer value passed to any module that takes a number of threads parameter script.permissions Set chmod command security bits on generated scripts (Ex. 770) script.timeout Integer, time (minutes) before worker scripts times out.","title":"script"},{"location":"Configuration/#seqfilevalidator","text":"Property Description seqFileValidator.requireEqualNumPairs Options: Y/N. default Y. seqFileValidator.seqMaxLen maximum number of bases per read seqFileValidator.seqMinLen minimum number of bases per read","title":"seqFileValidator"},{"location":"Configuration/#trimprimers","text":"Property Description trimPrimers.filePath Path to file containing one primer sequence per line. trimPrimers.requirePrimer Options: Y/N. If Y, TrimPrimers will discard reads that do not include a primer sequence.","title":"trimPrimers"},{"location":"Configuration/#validation","text":"Property Description validation.compareOn Which columns in the expectation file should be used for the comparison. Options: name, size, md5. Default: use all columns in the expectation file. validation.disableValidation Turn off validation. No validation file output is produced. Options: Y/N. default: N validation.expectationFile File path to the table of expectations. If a directory is given, BioLockJ will look for a file named after the module being evaluated. validation.reportOn Which attributes of the file should be included in the validation report file. Options: name, size, md5 validation.sizeWithinPercent What percentage difference is permitted between an output file and its expectation. Options: any positive number validation.stopPipeline If enabled, the validation utlility will stop the pipeline if any module fails validation. Options: Y/N","title":"validation"},{"location":"Dependencies/","text":"BioLockJ requires Java 1.8+ and a Unix-like operating system such as Darwin/macOS . Dependencies are required by modules listed in the BioModule Function column. Users DO NOT NEED TO INSTALL dependencies if not interested in the listed modules. For example, if you intend to classify 16S samples with RDP and WGS samples with Kraken, do not install: Bowtie2, GNU Awk, GNU Gzip, MetaPhlAn2, Python, QIIME 1, or Vsearch. # Program Version BioModule Function Link 1 Bowtie2 2.3.2 Metaphlan2Classifier : Build reference indexes download 2 GNU Awk 4.0.2 AwkFastaConverter : Convert Fastq to Fasta BuildQiimeMapping : Format metadata as QIIME mapping QiimeClosedRefClassifier : Build batch mapping files download 3 GNU Gzip 1.5 AwkFastaConverter : Decompress .gz files Gunzipper : Decompress .gz files download 4 Kraken 0.10.5-beta KrakenClassifier : Report WGS taxonomic summary download 5 MetaPhlAn2 2.0 Metaphlan2Classifier : Report WGS taxonomic summary (WGS) download 6 Python 2.7.12 BuildQiimeMapping : Run validate_mapping_file.py MergeQiimeOtuTables : Run merge_otu_tables.py QiimeClosedRefClassifier : Run pick_closed_reference_otus.py QiimeDeNovoClassifier : Run pick_de_novo_otus.py QiimeOpenRefClassifier : Run pick_open_reference_otus.py QiimeClassifier : Run add_alpha_to_mapping_file.py, add_qiime_labels.py, alpha_diversity.py, filter_otus_from_otu_table.py, print_qiime_config.py, and summarize_taxa.py Metaphlan2Classifier : Run metaphlan2.py download 7 PEAR 0.9.8 Paired-End reAd merger PearMergeReads Merge paired Fastq files since some classifiers ( RDP & QIIME ) will not accept paired reads. download 8 QIIME 1 1.9.1 Quantitative Insights Into Microbial Ecology BuildQiimeMapping : Validate QIIME mapping MergeQiimeOtuTables : Merge otu_table.biom files QiimeClosedRefClassifier : Pick OTUs by reference QiimeDeNovoClassifier : Pick OTUs by clustering QiimeOpenRefClassifier : Pick OTUs by reference and clustering QiimeClassifier : Report 16S taxonomic summary download 9 R 3.5.0 R_CalculateStats : Statistical modeling R_PlotPvalHistograms : Plot p-value histograms for each reportable metadata field R_PlotOtus : Build OTU-metadata boxplots and scatterplots R_PlotMds : Plot by top MDS axis R_PlotEffectSize : Build barplot of effect magnetude by OTU/taxa download 10 R-coin 1.2 COnditional Inference procedures in a permutatioN test framework R_CalculateStats : Compute exact Wilcox_test p-values download 11 R-ggpubr 0.1.8 R_PlotPvalHistograms : Set color palette R_PlotMds : Set color palette R_PlotEffectSize : Set color palette download 12 R-Kendall 2.2 R_CalculateStats : Compute rank correlation p-values for continuous data types download 13 R-properties 0.0-9 R_Module : Reads in the MASTER configuration properties file from the pipeline root directory download 14 R-stringr 1.2.0 R_Module : For string manipulation for handling Configuration properties download 15 R-vegan 2.5-2 R_PlotMds : Ordination methods, diversity analysis and other functions for ecologists. download 16 RDP 2.12 Ribosomal Database Project RdpClassifier : Report 16S taxonomic summary download 17 Vsearch 2.4.3 QiimeDeNovoClassifier : Chimera detection QiimeOpenRefClassifier : Chimera detection download Version Dependencies The Version column contains the version tested during BioLockJ development, but other versions can often be substituted. Major releases (such as Python 2 vs. Python 3) contain API changes that will not integrate with the current BioLockJ code. Application APIs often change over time, so not all versions are supported. For example, Bowtie2 did not add the large index functionality until version 2.3.2.","title":"Dependencies"},{"location":"Dependencies/#version-dependencies","text":"The Version column contains the version tested during BioLockJ development, but other versions can often be substituted. Major releases (such as Python 2 vs. Python 3) contain API changes that will not integrate with the current BioLockJ code. Application APIs often change over time, so not all versions are supported. For example, Bowtie2 did not add the large index functionality until version 2.3.2.","title":"Version Dependencies"},{"location":"Example-Pipeline/","text":"In our example analysis, we investigate the differences between the microbiome of 20 rural and 20 recently urbanized subjects from the Chinese province of Hunan. For more information on this dataset, please review the analysis Fodor Lab published in the Sep 2017 issue of the journal Microbiome: https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-017-0338-7 Step 1: Prepare BioLockJ Config File The BioLockJ project Config chinaKrakenFullDB.properties lists 5 BioModules to run (lines 3-7) + 13 properties: #BioModule biolockj.module.implicit.RegisterNumReads #BioModule biolockj.module.classifier.wgs.KrakenClassifier #BioModule biolockj.module.report.taxa.NormalizeTaxaTables #BioModule biolockj.module.report.r.R_PlotPvalHistograms #BioModule biolockj.module.report.r.R_PlotOtus In addition to the 5 listed BioModules, 4 additional implicit BioModules will also run: Mod# Module Description 1 ImportMetadata Always run 1st (for all pipelines) 2 KrakenParser Always run after KrakenClassifier 3 AddMetadataToOtuTables Always run just before the 1st R module 4 CalculateStats Always run as the 1st R module. Key properties: Line# Property Description 08 cluster.jobHeader Each script will run on 1 node, 16 cores, and 128GB RAM for up to 30 minutes 10 pipeline.defaultProps Default config file defines most properties \u2013 in this case copperhead.properties 12 input.dirPaths Directory path containing 40 gzipped whole genome sequencing (WGS) fastq files 18 metadata.filePath Metadata file path: chinaMetadata.tsv BioLockJ must associate sequence files in input.dirPaths with the correct metadata row. This is done by matching sequence file names to the 1st column in the metadata file. If the Sample ID is not found in your file names, the file names must be updated. Use the following properties to ignore a file prefix or suffix when matching the sample IDs. input.suffixFw input.suffixRv input.trimPrefix input.trimSuffix Sample IDs from 1st column of the metadata file: 081A, 082A, 083A...etc. Sequence file names: 081A_R1.fq.gz, 082A_R1.fq.gz, 083A_R1.fq.gz...etc. The default Config file, copperhead.properties, has its own default Config file standard.properties which defines the property input.suffixFw=_R1 . As a result, all characters starting with (and including) \u201c_R1\u201d are ignored when matching the file name to the metadata sample ID. Step 2: Run BioLockJ Pipeline > biolockj ~/chinaKrakenFullDB.properties Look in the BioLockJ pipeline output directory defined by $BLJ_PROJ for a new pipeline directory named after the property file + today\u2019s date: ~/projects/chinaKrakenFullDB_2018Apr09 The 5 configured modules have run in order, with the addition of 2 implicit modules (1st and last) which are added to all pipelines automatically. The biolockjComplete file indicates the pipeline ran successfully. Step 3: Review Pipeline Summary Run the blj_summary command to review the pipeline execution summary. > blj_summary Pipeline Summary Step 4: Download R Reports Run the blj_download command to get the command needed to download the analysis. > blj_download > rsync Step 5: Analyze R Reports Open downloadDir on your local filesystem to review the analysis. This directory contains: Output Description /temp Directory where R log files are saved if R script runs locally. /tables Directory containing the OTU tables. /local Directory where R script output is saved if R script runs locally and r.debug=Y . *.RData The saved R sessions for R modules run if r.saveRData=Y . chinaKrakenFullDB.log The pipeline Java log file. MAIN_*.R Each R script for each module that generated reports has been updated to run on your local filesystem. *.tsv files Spreadsheets containing p-value and R^2 statistics for each OTU in the taxonomy level. *.pdf files P-value histograms, and bar-charts or scatterplots for each OTU in the taxonomy level. Each R module generates a report for each report.taxonomyLevel configured: Open chinaKrakenFullDB_Log10_genus.pdf The report begins with the unadjusted P-Value Distributions: Since r.numHistogramBreaks=20 so the 1st bar represents the p-values < 0.05. The ruralUrban attribute appears significant, as indicated by the high number p-values < 0.05. For each OTU, a bar-chart or scatterplot is output with adjusted parametric and non-parametric p-values formatted using in the plot header. The p-value format is defined by r.pValFormat . The p-adjust method is defined by rStats.pAdjustMethod . P-values that meet the r.pvalCutoff threshold are highlighted with r.colorHighlight .","title":"Example Pipeline"},{"location":"Example-Pipeline/#step-1-prepare-biolockj-config-file","text":"The BioLockJ project Config chinaKrakenFullDB.properties lists 5 BioModules to run (lines 3-7) + 13 properties: #BioModule biolockj.module.implicit.RegisterNumReads #BioModule biolockj.module.classifier.wgs.KrakenClassifier #BioModule biolockj.module.report.taxa.NormalizeTaxaTables #BioModule biolockj.module.report.r.R_PlotPvalHistograms #BioModule biolockj.module.report.r.R_PlotOtus In addition to the 5 listed BioModules, 4 additional implicit BioModules will also run: Mod# Module Description 1 ImportMetadata Always run 1st (for all pipelines) 2 KrakenParser Always run after KrakenClassifier 3 AddMetadataToOtuTables Always run just before the 1st R module 4 CalculateStats Always run as the 1st R module. Key properties: Line# Property Description 08 cluster.jobHeader Each script will run on 1 node, 16 cores, and 128GB RAM for up to 30 minutes 10 pipeline.defaultProps Default config file defines most properties \u2013 in this case copperhead.properties 12 input.dirPaths Directory path containing 40 gzipped whole genome sequencing (WGS) fastq files 18 metadata.filePath Metadata file path: chinaMetadata.tsv BioLockJ must associate sequence files in input.dirPaths with the correct metadata row. This is done by matching sequence file names to the 1st column in the metadata file. If the Sample ID is not found in your file names, the file names must be updated. Use the following properties to ignore a file prefix or suffix when matching the sample IDs. input.suffixFw input.suffixRv input.trimPrefix input.trimSuffix Sample IDs from 1st column of the metadata file: 081A, 082A, 083A...etc. Sequence file names: 081A_R1.fq.gz, 082A_R1.fq.gz, 083A_R1.fq.gz...etc. The default Config file, copperhead.properties, has its own default Config file standard.properties which defines the property input.suffixFw=_R1 . As a result, all characters starting with (and including) \u201c_R1\u201d are ignored when matching the file name to the metadata sample ID.","title":"Step 1: Prepare BioLockJ Config File"},{"location":"Example-Pipeline/#step-2-run-biolockj-pipeline","text":"> biolockj ~/chinaKrakenFullDB.properties Look in the BioLockJ pipeline output directory defined by $BLJ_PROJ for a new pipeline directory named after the property file + today\u2019s date: ~/projects/chinaKrakenFullDB_2018Apr09 The 5 configured modules have run in order, with the addition of 2 implicit modules (1st and last) which are added to all pipelines automatically. The biolockjComplete file indicates the pipeline ran successfully.","title":"Step 2: Run BioLockJ Pipeline"},{"location":"Example-Pipeline/#step-3-review-pipeline-summary","text":"Run the blj_summary command to review the pipeline execution summary. > blj_summary Pipeline Summary","title":"Step 3: Review Pipeline Summary"},{"location":"Example-Pipeline/#step-4-download-r-reports","text":"Run the blj_download command to get the command needed to download the analysis. > blj_download > rsync","title":"Step 4: Download R Reports"},{"location":"Example-Pipeline/#step-5-analyze-r-reports","text":"Open downloadDir on your local filesystem to review the analysis. This directory contains: Output Description /temp Directory where R log files are saved if R script runs locally. /tables Directory containing the OTU tables. /local Directory where R script output is saved if R script runs locally and r.debug=Y . *.RData The saved R sessions for R modules run if r.saveRData=Y . chinaKrakenFullDB.log The pipeline Java log file. MAIN_*.R Each R script for each module that generated reports has been updated to run on your local filesystem. *.tsv files Spreadsheets containing p-value and R^2 statistics for each OTU in the taxonomy level. *.pdf files P-value histograms, and bar-charts or scatterplots for each OTU in the taxonomy level. Each R module generates a report for each report.taxonomyLevel configured:","title":"Step 5: Analyze R Reports"},{"location":"Example-Pipeline/#open-chinakrakenfulldb_log10_genuspdf","text":"The report begins with the unadjusted P-Value Distributions: Since r.numHistogramBreaks=20 so the 1st bar represents the p-values < 0.05. The ruralUrban attribute appears significant, as indicated by the high number p-values < 0.05. For each OTU, a bar-chart or scatterplot is output with adjusted parametric and non-parametric p-values formatted using in the plot header. The p-value format is defined by r.pValFormat . The p-adjust method is defined by rStats.pAdjustMethod . P-values that meet the r.pvalCutoff threshold are highlighted with r.colorHighlight .","title":"Open chinaKrakenFullDB_Log10_genus.pdf"},{"location":"FAQ/","text":"Q: If biolockj indicates that my pipeline started successfully, but the pipeline root directory is not created, how do I debug the root cause of the failure? A: Generally, errors are output to the pipeline log file and documented in the notification email, but invalid configuration settings may cause a fatal error to occur before the pipeline directory is created. In this scenario, look in your $HOME directory for a file name that starts with \"biolockj_FATAL_ERROR_\". Verify you are running Java 1.8+ java -version Look in the error message found in $HOME/biolockj_FATAL_ERROR_* for a reference to one of your Config file parameters, the most common culprit is: pipeline.defaultProps $BLJ_PROJ misconfigured in /script/blj_config Q: How should I configure input properties for a demultiplexed dataset? A: Name the sequence files using the Sample IDs listed in your metadata file. Sequence file names containing a prefix or suffix (in addition the Sample ID) can be used as long as there is a unique character string that can be used to identify the boundary between the Sample ID and its prefix or suffix. These values can be set via the input.trimPrefix & input.trimSuffix properties. Set input.trimPrefix to a character string that precedes the sample ID for all samples Set input.trimSuffix to a character string that comes after the sample ID for all samples If a single prefix or suffix identifier cannot be used for all samples, the file names must be updated so that a universal prefix or suffix identifier can be used. Example Sample IDs = mbs1, mbs2, mbs3, mbs4 Example File names + gut_mbs1.fq.gz + gut_mbs2.fq.gz + oral_mbs3.fq + oral_mbs4.fq Config Properties + input.trimPrefix =_ + input.trimSuffix =.fq All characters before (and including) the 1st \"_\" in the file name are trimmed All characters after (and including) the 1st \".fq\" in the file name are trimmed BioLockJ automatically trims extensions \".fasta\" and \".fastq\" as if configured in input.trimSuffix . Q: How do I configure my pipeline for multiplexed data? A: BioLockJ automatically adds the Demultiplexer as the 2nd module - after ImportMetadata - when processing multiplexed data. The Demultiplexer requires that the sequence headers contain either the Sample ID or an identifying barcode. Optionally, the barcode can be contained in the sequence itself. If your data does not conform to one of the following scenarios you will need to pre-process your sequence data to conform to a valid format. If samples are not identified by sample ID in the sequence headers: Set demux.strategy =id_in_header Set input.trimPrefix to a character string that precedes the sample ID for all samples . Set input.trimSuffix to a character string that comes after the sample ID for all samples . Sample IDs = mbs1, mbs2, mbs3, mbs4 Scenario 1: Your multiplexed files include Sample IDs in the fastq sequence headers @mbs1_134_M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0 @mbs2_12_M02825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0 @mbs3_551_M03825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0 @mbs4_1234_M04825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0 Required Config + input.trimPrefix =@ + input.trimSuffix =_ All characters before (and including) the 1st \"@\" in the sequence header are trimmed All characters after (and including) the 1st \"_\" in the sequence header are trimmed If samples are identified by barcode (in the header or sequence): Set demux.strategy =barcode_in_header or demux.strategy =barcode_in_seq Set metadata.filePath to metadata file path. Set metadata.barcodeColumn to the barcode column name. If the metadata barcodes are listed as reverse compliments, set demux.barcodeUseReverseCompliment =Y. The metadata file must be prepared by adding a unique sequence barcode in the metadata.barcodeColumn column. This information is often available in a mapping file provided by the sequencing center that produced the raw data. Metadata file ID BarcodeColumn mbs1 GAGGCATGACTGGATA mbs2 NAGGCATATTTGCACA mbs3 GACCCATGACTGCATA mbs4 TACCCAGCACCGCTTA Scenario 2: Your multiplexed files include a barcode in the headers @M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0:GAGGCATGACTGGATA @M01825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0:NAGGCATATTTGCACA @M01825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0:GACCCATGACTGCATA @M01825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0:TACCCAGCACCGCTTA Required Config + demux.strategy =barcode_in_header + metadata.barcodeColumn =BarcodeColumn + metadata.filePath = Scenario 3: Your multiplexed files include a barcode in the sequences >M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0: GAGGCATGACTGGATATATACATACTGAGGCATGACTACTTACTATAAGGCTTACTGACTGGTTACTGACTGGGAGGCATGACTACTTACTATAA >M01825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0: CAGGCATATTTGCACACTAGAGGCAAGTTACTGACTGGATATACTGAGGCATGGGAGGCATGACTCTATAAGGCTTACTGACTGGTTACTGACTG >M01825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0: CCATGAGACCTGCATA CCATGAGACCTGCATACACTGTGGGAGGCATGACTCACTATAAACTACTACTGACTGGATATACTGAGGCATACTGACTGGTTACTTATAAGGCT >M01825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0:TACCCAGCACCGCTTA TACCCAGCACCGCTTCCTTGACTTGGGAGGCATGACTCACTATAAACTACTACTGACTGGATATACTGAGGCATACTGACTGGTTACTTATAAGG","title":"FAQ"},{"location":"FAQ/#q-if-biolockj-indicates-that-my-pipeline-started-successfully-but-the-pipeline-root-directory-is-not-created-how-do-i-debug-the-root-cause-of-the-failure","text":"A: Generally, errors are output to the pipeline log file and documented in the notification email, but invalid configuration settings may cause a fatal error to occur before the pipeline directory is created. In this scenario, look in your $HOME directory for a file name that starts with \"biolockj_FATAL_ERROR_\". Verify you are running Java 1.8+ java -version Look in the error message found in $HOME/biolockj_FATAL_ERROR_* for a reference to one of your Config file parameters, the most common culprit is: pipeline.defaultProps $BLJ_PROJ misconfigured in /script/blj_config","title":"Q: If biolockj indicates that my pipeline started successfully, but the pipeline root directory is not created, how do I debug the root cause of the failure?"},{"location":"FAQ/#q-how-should-i-configure-input-properties-for-a-demultiplexed-dataset","text":"A: Name the sequence files using the Sample IDs listed in your metadata file. Sequence file names containing a prefix or suffix (in addition the Sample ID) can be used as long as there is a unique character string that can be used to identify the boundary between the Sample ID and its prefix or suffix. These values can be set via the input.trimPrefix & input.trimSuffix properties. Set input.trimPrefix to a character string that precedes the sample ID for all samples Set input.trimSuffix to a character string that comes after the sample ID for all samples If a single prefix or suffix identifier cannot be used for all samples, the file names must be updated so that a universal prefix or suffix identifier can be used.","title":"Q: How should I configure input properties for a demultiplexed dataset?"},{"location":"FAQ/#example","text":"Sample IDs = mbs1, mbs2, mbs3, mbs4 Example File names + gut_mbs1.fq.gz + gut_mbs2.fq.gz + oral_mbs3.fq + oral_mbs4.fq Config Properties + input.trimPrefix =_ + input.trimSuffix =.fq All characters before (and including) the 1st \"_\" in the file name are trimmed All characters after (and including) the 1st \".fq\" in the file name are trimmed BioLockJ automatically trims extensions \".fasta\" and \".fastq\" as if configured in input.trimSuffix .","title":"Example"},{"location":"FAQ/#q-how-do-i-configure-my-pipeline-for-multiplexed-data","text":"A: BioLockJ automatically adds the Demultiplexer as the 2nd module - after ImportMetadata - when processing multiplexed data. The Demultiplexer requires that the sequence headers contain either the Sample ID or an identifying barcode. Optionally, the barcode can be contained in the sequence itself. If your data does not conform to one of the following scenarios you will need to pre-process your sequence data to conform to a valid format.","title":"Q: How do I configure my pipeline for multiplexed data?"},{"location":"FAQ/#if-samples-are-not-identified-by-sample-id-in-the-sequence-headers","text":"Set demux.strategy =id_in_header Set input.trimPrefix to a character string that precedes the sample ID for all samples . Set input.trimSuffix to a character string that comes after the sample ID for all samples . Sample IDs = mbs1, mbs2, mbs3, mbs4 Scenario 1: Your multiplexed files include Sample IDs in the fastq sequence headers @mbs1_134_M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0 @mbs2_12_M02825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0 @mbs3_551_M03825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0 @mbs4_1234_M04825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0 Required Config + input.trimPrefix =@ + input.trimSuffix =_ All characters before (and including) the 1st \"@\" in the sequence header are trimmed All characters after (and including) the 1st \"_\" in the sequence header are trimmed","title":"If samples are not identified by sample ID in the sequence headers:"},{"location":"FAQ/#if-samples-are-identified-by-barcode-in-the-header-or-sequence","text":"Set demux.strategy =barcode_in_header or demux.strategy =barcode_in_seq Set metadata.filePath to metadata file path. Set metadata.barcodeColumn to the barcode column name. If the metadata barcodes are listed as reverse compliments, set demux.barcodeUseReverseCompliment =Y. The metadata file must be prepared by adding a unique sequence barcode in the metadata.barcodeColumn column. This information is often available in a mapping file provided by the sequencing center that produced the raw data. Metadata file ID BarcodeColumn mbs1 GAGGCATGACTGGATA mbs2 NAGGCATATTTGCACA mbs3 GACCCATGACTGCATA mbs4 TACCCAGCACCGCTTA Scenario 2: Your multiplexed files include a barcode in the headers @M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0:GAGGCATGACTGGATA @M01825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0:NAGGCATATTTGCACA @M01825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0:GACCCATGACTGCATA @M01825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0:TACCCAGCACCGCTTA Required Config + demux.strategy =barcode_in_header + metadata.barcodeColumn =BarcodeColumn + metadata.filePath = Scenario 3: Your multiplexed files include a barcode in the sequences >M01825:384:000000000-BCYPK:1:2106:23543:1336 1:N:0: GAGGCATGACTGGATATATACATACTGAGGCATGACTACTTACTATAAGGCTTACTGACTGGTTACTGACTGGGAGGCATGACTACTTACTATAA >M01825:384:000000000-BCYPK:1:1322:23543:1336 1:N:0: CAGGCATATTTGCACACTAGAGGCAAGTTACTGACTGGATATACTGAGGCATGGGAGGCATGACTCTATAAGGCTTACTGACTGGTTACTGACTG >M01825:384:000000000-BCYPK:1:1123:23543:1336 1:N:0: CCATGAGACCTGCATA CCATGAGACCTGCATACACTGTGGGAGGCATGACTCACTATAAACTACTACTGACTGGATATACTGAGGCATACTGACTGGTTACTTATAAGGCT >M01825:384:000000000-BCYPK:1:9872:23543:1336 1:N:0:TACCCAGCACCGCTTA TACCCAGCACCGCTTCCTTGACTTGGGAGGCATGACTCACTATAAACTACTACTGACTGGATATACTGAGGCATACTGACTGGTTACTTATAAGG","title":"If samples are identified by barcode (in the header or sequence):"},{"location":"Failure-Recovery/","text":"Failed Pipelines Failed pipelines can be restarted to save the progress made by successfully completed modules. To restart a failed pipeline, add -r parameter: biolockj -r <pipeline root> Check the BioLockJ execution summary after any failure occurs via the blj_summary command. blj_summary Review the BioLockJ log file in your pipeline root directory. This is the most complete source of information on pipeline execution and may contain useful error messages that help resolve the root cause of the failure. If your pipeline directory is not created, you likely have invalid file paths or missing/invalid properties Check the FATAL_ERROR file your $HOME directory. This is where BioLockJ saves error messages for failures occurring prior to the creation of your pipeline root directory. cat \"$HOME/biolockj_FATAL_ERROR_*\" The most common culprits are: pipeline.defaultProps Missing/invalid $BLJ_PROJ directory set by the install script echo $BLJ_PROJ Failures that occur in a module/script are not logged to the Java log file since these run outside of the Java application. The failed module directory may have an indicator file that gives either the sample ID or bash script line that failed. If the failed module was running a bash script on the cluster, check the module/qsub directory for the cluster job output and error files which may contain additional information.","title":"Failure Recovery"},{"location":"Failure-Recovery/#failed-pipelines","text":"Failed pipelines can be restarted to save the progress made by successfully completed modules. To restart a failed pipeline, add -r parameter: biolockj -r <pipeline root> Check the BioLockJ execution summary after any failure occurs via the blj_summary command. blj_summary Review the BioLockJ log file in your pipeline root directory. This is the most complete source of information on pipeline execution and may contain useful error messages that help resolve the root cause of the failure. If your pipeline directory is not created, you likely have invalid file paths or missing/invalid properties Check the FATAL_ERROR file your $HOME directory. This is where BioLockJ saves error messages for failures occurring prior to the creation of your pipeline root directory. cat \"$HOME/biolockj_FATAL_ERROR_*\" The most common culprits are: pipeline.defaultProps Missing/invalid $BLJ_PROJ directory set by the install script echo $BLJ_PROJ Failures that occur in a module/script are not logged to the Java log file since these run outside of the Java application. The failed module directory may have an indicator file that gives either the sample ID or bash script line that failed. If the failed module was running a bash script on the cluster, check the module/qsub directory for the cluster job output and error files which may contain additional information.","title":"Failed Pipelines"},{"location":"Getting-Started/","text":"1. Complete Program Installation 2. Configure Program Input See Configuration 3. Start Your 1st Pipeline To start your pipeline, pass your configuration file path $CONFIG_PATH to the biolockj command: biolockj $CONFIG_PATH Verify biolockj command output is accurate: BioLockJ JAR file path is valid BioLockJ Configuration file path is valid Verify message \"BioLockJ started successfully!\" is printed. 4. Check Pipeline Status Check pipeline progress by tailing the pipeline Java log file with blj_go and blj_log : blj_go # Go to your newest pipeline blj_log # Tail pipeline log file Verify java is running: ps -u $USER # psu # PID TTY TIME CMD # 16499 pts/45 00:36:21 java # 188510 pts/45 00:00:00 bash # ... If running modules on the cluster, check status of BioLockJ scripts on the job queue (command depends on job queue implementation): qstat -u $USER Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 530799.cph-m1.uncc.edu msioda cph_shor 04.0_PearMergeRe 44312 1 8 64gb 01:00:00 C 00:19:23 530800.cph-m1.uncc.edu msioda cph_shor 04.1_PearMergeRe 36453 1 8 64gb 01:00:00 R 00:05:23 5. Investigate Failed Pipelines Failed pipelines can be restarted to save the progress made by successful modules. See Failure Recovery for more information. Failure Recovery should be avoided until you have successfully completed your 1st pipeline.","title":"Getting Started"},{"location":"Getting-Started/#1-complete-program-installation","text":"","title":"1. Complete Program Installation"},{"location":"Getting-Started/#2-configure-program-input","text":"See Configuration","title":"2. Configure Program Input"},{"location":"Getting-Started/#3-start-your-1st-pipeline","text":"To start your pipeline, pass your configuration file path $CONFIG_PATH to the biolockj command: biolockj $CONFIG_PATH Verify biolockj command output is accurate: BioLockJ JAR file path is valid BioLockJ Configuration file path is valid Verify message \"BioLockJ started successfully!\" is printed.","title":"3. Start Your 1st Pipeline"},{"location":"Getting-Started/#4-check-pipeline-status","text":"Check pipeline progress by tailing the pipeline Java log file with blj_go and blj_log : blj_go # Go to your newest pipeline blj_log # Tail pipeline log file Verify java is running: ps -u $USER # psu # PID TTY TIME CMD # 16499 pts/45 00:36:21 java # 188510 pts/45 00:00:00 bash # ... If running modules on the cluster, check status of BioLockJ scripts on the job queue (command depends on job queue implementation): qstat -u $USER Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 530799.cph-m1.uncc.edu msioda cph_shor 04.0_PearMergeRe 44312 1 8 64gb 01:00:00 C 00:19:23 530800.cph-m1.uncc.edu msioda cph_shor 04.1_PearMergeRe 36453 1 8 64gb 01:00:00 R 00:05:23","title":"4. Check Pipeline Status"},{"location":"Getting-Started/#5-investigate-failed-pipelines","text":"Failed pipelines can be restarted to save the progress made by successful modules. See Failure Recovery for more information. Failure Recovery should be avoided until you have successfully completed your 1st pipeline.","title":"5. Investigate Failed Pipelines"},{"location":"Installation/","text":"1. Download the latest release & unpack the tarball. tar -zxf BioLockJ-1.2.0-beta Put the folder to wherever you like to keep executables. If you choose to download the source code, you will need to compile it by running ant from the resources folder. 2. Run the install script The install script updates the $USER bash profile to call blj_config . See Commands for a full description of blj_config ./install # Saved backup: /users/msioda/.bash_profile~ # Saved profile: /users/msioda/.bash_profile # BioLockJ installation complete! This will add the required variables to your path when you start your next session. To use BioLockJ in the same session, run source ~/.bash_profile . 3. Install the software Dependencies required by the modules you wish to include in your pipeline. Notes Environments The main BioLockJ program can be used in these environments: a local machine with a unix-like system any machine running docker * a cluster, running a scheduler such as torque AWS cloud computing * (* The launch scripts will still be run from your local machine.) The launch process requires a unix-like environment. This includes linux, macOS, or an ubuntu environment running on Windows. If using docker , you will need to run the install script to create the variables used by the launch scripts, even though the main BioLockJ program will run within the biolockj_controller container. If using AWS , you will need to run the install script to create the variables used by the launch scripts, even though the main BioLockJ program will run on AWS. If you are using BioLockJ on a shared system where another user has already installed BioLockJ, you will need to run the install script to create the required variables in your own user profile.","title":"Installation"},{"location":"Installation/#1-download-the-latest-release-unpack-the-tarball","text":"tar -zxf BioLockJ-1.2.0-beta Put the folder to wherever you like to keep executables. If you choose to download the source code, you will need to compile it by running ant from the resources folder.","title":"1. Download the latest release &amp; unpack the tarball."},{"location":"Installation/#2-run-the-install-script","text":"The install script updates the $USER bash profile to call blj_config . See Commands for a full description of blj_config ./install # Saved backup: /users/msioda/.bash_profile~ # Saved profile: /users/msioda/.bash_profile # BioLockJ installation complete! This will add the required variables to your path when you start your next session. To use BioLockJ in the same session, run source ~/.bash_profile .","title":"2. Run the install script"},{"location":"Installation/#3-install-the-software-dependencies-required-by-the-modules-you-wish-to-include-in-your-pipeline","text":"","title":"3. Install the software Dependencies required by the modules you wish to include in your pipeline."},{"location":"Installation/#notes","text":"","title":"Notes"},{"location":"Installation/#environments","text":"The main BioLockJ program can be used in these environments: a local machine with a unix-like system any machine running docker * a cluster, running a scheduler such as torque AWS cloud computing * (* The launch scripts will still be run from your local machine.) The launch process requires a unix-like environment. This includes linux, macOS, or an ubuntu environment running on Windows. If using docker , you will need to run the install script to create the variables used by the launch scripts, even though the main BioLockJ program will run within the biolockj_controller container. If using AWS , you will need to run the install script to create the variables used by the launch scripts, even though the main BioLockJ program will run on AWS. If you are using BioLockJ on a shared system where another user has already installed BioLockJ, you will need to run the install script to create the required variables in your own user profile.","title":"Environments"},{"location":"The-Metadata-File/","text":"The config file gives information about the pipeline , sometimes specific to one module . The metadata file gives information specific to each sample . This might include membership in sample groups for statistical tests, or barcode sequences for demultiplexing or the filenames from the input dirs to associate with each sample.","title":"Metadata"},{"location":"Validation/","text":"Summary Description: Validation checks whether the output files of a pipeline match the expectation . Options: validation.compareOn validation.disableValidation validation.expectationFile validation.reportOn validation.sizeWithinPercent validation.stopPipeline The validation utility creates a table for the output of each module where it reports the file name, size and md5. These tables are saved in the validation folder; the validation folder generated by a pipeline can be used as the expectations when re-running the same pipeline. If there are no expectations, these values are reported in the validation folder. If there are expectations, these values are reported and compared against the expected values; the result of the comparison is reported as either PASS or FAIL for each file. If validation.stopPipeline=Y , the validation utility will halt the pipeline if any outputs FAIL to meet expectations, otherwise the result is reported and the pipeline moves forward. Soft Validation Many components of a pipeline have the potential for tiny variation: maybe a date is stored in the output, or a reported confidence level is based on a random sampling. With these tiny variations, the file is practically the same, but it will FAIL md5 validation. The file might also be a few bytes bigger or smaller, so it will also FAIL size validation. \"Soft validation\" is the practice of allowing some wiggle room. If the config file gives validation.sizeWithinPercent=1 , then an output file will PASS size validation as long as it is within 1.0% of the expected file size. By default, this value is 0, and a file must be exactly the expected size to pass size validation. Expectations Give the file path to the expectation file using validation.expectationFile=/path/to/saved/validation . This path can either point to a tab-delimited table giving the expectations for a single module, or it can point to a folder, in which case BioLockJ assumes that a file within this folder has a name that matches the module being validated. When validating an entire pipeline, the expectation file for all modules can be passed with a single file path. The validation folder created by a pipeline is designed to be used as this input. The expectation file format is: The expectation file is a tab-delimited table. The first row is column names. The first column (labeled \"name\") gives the file names. Optional column \"size\" gives the file size in bytes. Optional column \"md5\" gives the md5 string. Optional column \"MATCHED_EXPECTION\" is always ignored. The file should not have any other columns. Use cases The expectation is usually based on a previous run of the same pipeline. * Maybe some software has been updated and the results are not expected to change, but you have to re-do your analysis with the latest version to appease reviewers. * Maybe you added a filtering step. * Maybe you just want to change module 5, and you expect 1-4 to produce the same outputs they did last time. * Maybe this analysis has been published and the the original researcher made their pipeline available to you; you want to re-run it and know if the output you generated by running the pipeline is the same as what they had. The expectation can be set by hand. This is recommended for validation using name only, or soft validation using size only. This is a way to prevent a pipeline from continuing after it is effectively doomed. For example: Maybe module 5 is a resource-intensive classifier, and modules 1-4 are processing and filtering steps ending with the SeqFileValidator. If modules 1-4 filter out too much, you might not want to move forward with module 5 until you've made adjustments to the earlier modules. You could create an expectation file for module 4, that just lists the names of the files and their pre-filtering file size (in bytes), and set validation.sizeWithinPercent=80 and SeqFileValidator.stopPipeline=Y . With this, the pipeline will stop if any of those files are not in the module 4 output or if any of them have been reduced by more than 80%. The output file names are predictable if you've ever seen output from that module before.","title":"Validation"},{"location":"Validation/#summary","text":"Description: Validation checks whether the output files of a pipeline match the expectation . Options: validation.compareOn validation.disableValidation validation.expectationFile validation.reportOn validation.sizeWithinPercent validation.stopPipeline The validation utility creates a table for the output of each module where it reports the file name, size and md5. These tables are saved in the validation folder; the validation folder generated by a pipeline can be used as the expectations when re-running the same pipeline. If there are no expectations, these values are reported in the validation folder. If there are expectations, these values are reported and compared against the expected values; the result of the comparison is reported as either PASS or FAIL for each file. If validation.stopPipeline=Y , the validation utility will halt the pipeline if any outputs FAIL to meet expectations, otherwise the result is reported and the pipeline moves forward.","title":"Summary"},{"location":"Validation/#soft-validation","text":"Many components of a pipeline have the potential for tiny variation: maybe a date is stored in the output, or a reported confidence level is based on a random sampling. With these tiny variations, the file is practically the same, but it will FAIL md5 validation. The file might also be a few bytes bigger or smaller, so it will also FAIL size validation. \"Soft validation\" is the practice of allowing some wiggle room. If the config file gives validation.sizeWithinPercent=1 , then an output file will PASS size validation as long as it is within 1.0% of the expected file size. By default, this value is 0, and a file must be exactly the expected size to pass size validation.","title":"Soft Validation"},{"location":"Validation/#expectations","text":"Give the file path to the expectation file using validation.expectationFile=/path/to/saved/validation . This path can either point to a tab-delimited table giving the expectations for a single module, or it can point to a folder, in which case BioLockJ assumes that a file within this folder has a name that matches the module being validated. When validating an entire pipeline, the expectation file for all modules can be passed with a single file path. The validation folder created by a pipeline is designed to be used as this input. The expectation file format is: The expectation file is a tab-delimited table. The first row is column names. The first column (labeled \"name\") gives the file names. Optional column \"size\" gives the file size in bytes. Optional column \"md5\" gives the md5 string. Optional column \"MATCHED_EXPECTION\" is always ignored. The file should not have any other columns.","title":"Expectations"},{"location":"Validation/#use-cases","text":"The expectation is usually based on a previous run of the same pipeline. * Maybe some software has been updated and the results are not expected to change, but you have to re-do your analysis with the latest version to appease reviewers. * Maybe you added a filtering step. * Maybe you just want to change module 5, and you expect 1-4 to produce the same outputs they did last time. * Maybe this analysis has been published and the the original researcher made their pipeline available to you; you want to re-run it and know if the output you generated by running the pipeline is the same as what they had. The expectation can be set by hand. This is recommended for validation using name only, or soft validation using size only. This is a way to prevent a pipeline from continuing after it is effectively doomed. For example: Maybe module 5 is a resource-intensive classifier, and modules 1-4 are processing and filtering steps ending with the SeqFileValidator. If modules 1-4 filter out too much, you might not want to move forward with module 5 until you've made adjustments to the earlier modules. You could create an expectation file for module 4, that just lists the names of the files and their pre-filtering file size (in bytes), and set validation.sizeWithinPercent=80 and SeqFileValidator.stopPipeline=Y . With this, the pipeline will stop if any of those files are not in the module 4 output or if any of them have been reduced by more than 80%. The output file names are predictable if you've ever seen output from that module before.","title":"Use cases"},{"location":"module/classifier/module.classifier/","text":"Classifier Package Modules in the biolockj.module.classifier package categorize micbrobial samples into Operational Taxonomic Units (OTUs) either by reference or with clustering algorithms. This package contains 2 sub-packages: module.classifier.r16s contains modules designed to classify 16S data. module.classifier.wgs contains modules designed to classify whole genome sequence data. Modules in these sub-packages extend the ClassifierModuleImpl class. ClassifierModuleImpl Description: Abstract implementation of the ClassifierModule interface that the other classifier modules extend to inherit standard functionality. Abstract modules cannot be included in the pipeline run order.","title":"Classifier Package"},{"location":"module/classifier/module.classifier/#classifier-package","text":"Modules in the biolockj.module.classifier package categorize micbrobial samples into Operational Taxonomic Units (OTUs) either by reference or with clustering algorithms. This package contains 2 sub-packages: module.classifier.r16s contains modules designed to classify 16S data. module.classifier.wgs contains modules designed to classify whole genome sequence data. Modules in these sub-packages extend the ClassifierModuleImpl class.","title":"Classifier Package"},{"location":"module/classifier/module.classifier/#classifiermoduleimpl","text":"Description: Abstract implementation of the ClassifierModule interface that the other classifier modules extend to inherit standard functionality. Abstract modules cannot be included in the pipeline run order.","title":"ClassifierModuleImpl"},{"location":"module/classifier/module.classifier.r16s/","text":"biolockj.module.classifier.r16s is a sub-package of module.classifier. Package modules extend ClassifierModuleImpl to cluster and classify 16S micbrobial samples for taxonomy assignment. QiimeClosedRefClassifier #BioModule biolockj.module.classifier.r16s.QiimeClosedRefClassifier Description: This module picks OTUs using a closed reference database and constructs an OTU table via the QIIME script pick_closed_reference_otus.py . Taxonomy is assigned using a pre-defined taxonomy map of reference sequence OTU to taxonomy. This is the fastest OTU picking method since samples can be processed in parallel batches. Before the QIIME script is run, batches are prepared in the temp directory, with each batch directory containing a fasta directory with script.batchSize fasta files and a QIIME mapping file, created with awk, called batchMapping.tsv for the batch of samples. Inherits from QiimeClassifier . Options: exe.awk QiimeDeNovoClassifier #BioModule biolockj.module.classifier.r16s.QiimeDeNovoClassifier Description: This module runs the QIIME pick_de_novo_otus.py script on all fasta sequence files in a single script since OTUs are assigned by a clustering algorithm. Additional parameters for this script are set using exe.classifierParams . If qiime.removeChimeras = \"Y\", vsearch is used to find chimeric sequences in the output and the QIIME script filter_otus_from_otu_table.py is run to remove them from ./output/otu_table.biom. Inherits from QiimeClassifier . Options: exe.vsearch exe.vsearchParams qiime.removeChimeras QiimeOpenRefClassifier #BioModule biolockj.module.classifier.r16s.QiimeOpenRefClassifier Description: This module runs the QIIME pick_open_reference_otus.py script on all fasta sequence files in a single script since clusters not identified in the reference database are assigned by a clustering algorithm. Additional parameters for this script are set using exe.classifierParams . If qiime.removeChimeras = \"Y\", vsearch is used to find chimeric sequences in the output and the QIIME script filter_otus_from_otu_table.py is run to remove them from ./output/otu_table.biom. Inherits from QiimeClassifier . Options: exe.vsearch exe.vsearchParams qiime.removeChimeras RdpClassifier #BioModule biolockj.module.classifier.r16s.RdpClassifier Description: Classify 16s samples with RDP . Options: exe.java rdp.db rdp.jar rdp.minThresholdScore See also: Typical QIIME Pipeline","title":"Module.classifier.r16s"},{"location":"module/classifier/module.classifier.r16s/#qiimeclosedrefclassifier","text":"#BioModule biolockj.module.classifier.r16s.QiimeClosedRefClassifier Description: This module picks OTUs using a closed reference database and constructs an OTU table via the QIIME script pick_closed_reference_otus.py . Taxonomy is assigned using a pre-defined taxonomy map of reference sequence OTU to taxonomy. This is the fastest OTU picking method since samples can be processed in parallel batches. Before the QIIME script is run, batches are prepared in the temp directory, with each batch directory containing a fasta directory with script.batchSize fasta files and a QIIME mapping file, created with awk, called batchMapping.tsv for the batch of samples. Inherits from QiimeClassifier . Options: exe.awk","title":"QiimeClosedRefClassifier"},{"location":"module/classifier/module.classifier.r16s/#qiimedenovoclassifier","text":"#BioModule biolockj.module.classifier.r16s.QiimeDeNovoClassifier Description: This module runs the QIIME pick_de_novo_otus.py script on all fasta sequence files in a single script since OTUs are assigned by a clustering algorithm. Additional parameters for this script are set using exe.classifierParams . If qiime.removeChimeras = \"Y\", vsearch is used to find chimeric sequences in the output and the QIIME script filter_otus_from_otu_table.py is run to remove them from ./output/otu_table.biom. Inherits from QiimeClassifier . Options: exe.vsearch exe.vsearchParams qiime.removeChimeras","title":"QiimeDeNovoClassifier"},{"location":"module/classifier/module.classifier.r16s/#qiimeopenrefclassifier","text":"#BioModule biolockj.module.classifier.r16s.QiimeOpenRefClassifier Description: This module runs the QIIME pick_open_reference_otus.py script on all fasta sequence files in a single script since clusters not identified in the reference database are assigned by a clustering algorithm. Additional parameters for this script are set using exe.classifierParams . If qiime.removeChimeras = \"Y\", vsearch is used to find chimeric sequences in the output and the QIIME script filter_otus_from_otu_table.py is run to remove them from ./output/otu_table.biom. Inherits from QiimeClassifier . Options: exe.vsearch exe.vsearchParams qiime.removeChimeras","title":"QiimeOpenRefClassifier"},{"location":"module/classifier/module.classifier.r16s/#rdpclassifier","text":"#BioModule biolockj.module.classifier.r16s.RdpClassifier Description: Classify 16s samples with RDP . Options: exe.java rdp.db rdp.jar rdp.minThresholdScore See also: Typical QIIME Pipeline","title":"RdpClassifier"},{"location":"module/classifier/module.classifier.wgs/","text":"Whole Genome Sequence Classifiers biolockj.module.classifier.wgs is a sub-package of module.classifier. Package modules categorize whole genome sequence micbrobial samples into Operational Taxonomic Units (OTUs) either by reference or with clustering algorithms. Humann2Classifier #BioModule biolockj.module.classifier.wgs.Humann2Classifier Description: Use the Biobakery HumanN2 program to generate the HMP Unified Metabolic Analysis Network. Options: humann2.disablePathAbundance humann2.disablePathCoverage humann2.disableGeneFamilies humann2.nuclDB humann2.protDB KrakenClassifier #BioModule biolockj.module.classifier.wgs.KrakenClassifier Description: Classify WGS samples with KRAKEN . Options: kraken.db Kraken2Classifier #BioModule biolockj.module.classifier.wgs.Kraken2Classifier Description: Classify WGS samples with KRAKEN 2 . Options: kraken2.db Metaphlan2Classifier #BioModule biolockj.module.classifier.wgs.Metaphlan2Classifier Description: Classify WGS samples with MetaPhlAn . Options: exe.python metaphlan2.db metaphlan2.mpa_pkl","title":"Whole Genome Sequence Classifiers"},{"location":"module/classifier/module.classifier.wgs/#whole-genome-sequence-classifiers","text":"biolockj.module.classifier.wgs is a sub-package of module.classifier. Package modules categorize whole genome sequence micbrobial samples into Operational Taxonomic Units (OTUs) either by reference or with clustering algorithms.","title":"Whole Genome Sequence Classifiers"},{"location":"module/classifier/module.classifier.wgs/#humann2classifier","text":"#BioModule biolockj.module.classifier.wgs.Humann2Classifier Description: Use the Biobakery HumanN2 program to generate the HMP Unified Metabolic Analysis Network. Options: humann2.disablePathAbundance humann2.disablePathCoverage humann2.disableGeneFamilies humann2.nuclDB humann2.protDB","title":"Humann2Classifier"},{"location":"module/classifier/module.classifier.wgs/#krakenclassifier","text":"#BioModule biolockj.module.classifier.wgs.KrakenClassifier Description: Classify WGS samples with KRAKEN . Options: kraken.db","title":"KrakenClassifier"},{"location":"module/classifier/module.classifier.wgs/#kraken2classifier","text":"#BioModule biolockj.module.classifier.wgs.Kraken2Classifier Description: Classify WGS samples with KRAKEN 2 . Options: kraken2.db","title":"Kraken2Classifier"},{"location":"module/classifier/module.classifier.wgs/#metaphlan2classifier","text":"#BioModule biolockj.module.classifier.wgs.Metaphlan2Classifier Description: Classify WGS samples with MetaPhlAn . Options: exe.python metaphlan2.db metaphlan2.mpa_pkl","title":"Metaphlan2Classifier"},{"location":"module/diy/module.diy/","text":"Do It Yourself - DIY Package DIY package The DIY package allows users to customize BioLockJ to fit their needs. GenMod #BioModule biolockj.module.diy.GenMod Description: Allows user to add external scripts into the BioLockJ pipeline. Options: genMod.launcher genMod.param genMod.scriptPath","title":"Do It Yourself - DIY Package"},{"location":"module/diy/module.diy/#do-it-yourself-diy-package","text":"","title":"Do It Yourself - DIY Package"},{"location":"module/diy/module.diy/#diy-package","text":"The DIY package allows users to customize BioLockJ to fit their needs.","title":"DIY package"},{"location":"module/diy/module.diy/#genmod","text":"#BioModule biolockj.module.diy.GenMod Description: Allows user to add external scripts into the BioLockJ pipeline. Options: genMod.launcher genMod.param genMod.scriptPath","title":"GenMod"},{"location":"module/implicit/module.implicit/","text":"biolockj.module.implicit modules are added to BioLockJ pipelines automatically if needed. Implicit modules are ignored if included in the Config file unless project.allowImplicitModules=Y This package contains the following sub-packages: module.implicit.parser contains ParserModule interface & ParserModuleImpl superclass. module.implicit.parser.r16s contains 16S parser modules. module.implicit.parser.wgs contains WGS parser modules. module.implicit.qiime contains QIIME Script wrappers. Demultiplexer (added by BioLockJ) #BioModule biolockj.module.implicit.ImportMetadata Description: Demultiplex samples into separate files for each sample. Options: demultiplexer.barcodeCutoff demultiplexer.barcodeRevComp demultiplexer.strategy metadata.filePath ImportMetadata (added by BioLockJ) #BioModule biolockj.module.implicit.ImportMetadata Description: Required 1st module in every pipeline. If metadata.filePath is undefined, a new metadata file will be created with only a single column \"SAMPLE_ID\". The imported file is converted to required BioLockJ metadata format: tab-delimited, with unique column headers, and empty cells are now populated with metadata.nullValue or \"NA\" if undefined. Options: metadata.columnDelim metadata.commentChar metadata.filePath metadata.nullValue RegisterNumReads (added by BioLockJ) #BioModule biolockj.module.implicit.RegisterNumReads Description: Add \"Num_Reads\" column to metadata file to document the total number of reads per sample. Options: report.numReads","title":"Module.implicit"},{"location":"module/implicit/module.implicit/#demultiplexer","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.ImportMetadata Description: Demultiplex samples into separate files for each sample. Options: demultiplexer.barcodeCutoff demultiplexer.barcodeRevComp demultiplexer.strategy metadata.filePath","title":"Demultiplexer"},{"location":"module/implicit/module.implicit/#importmetadata","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.ImportMetadata Description: Required 1st module in every pipeline. If metadata.filePath is undefined, a new metadata file will be created with only a single column \"SAMPLE_ID\". The imported file is converted to required BioLockJ metadata format: tab-delimited, with unique column headers, and empty cells are now populated with metadata.nullValue or \"NA\" if undefined. Options: metadata.columnDelim metadata.commentChar metadata.filePath metadata.nullValue","title":"ImportMetadata"},{"location":"module/implicit/module.implicit/#registernumreads","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.RegisterNumReads Description: Add \"Num_Reads\" column to metadata file to document the total number of reads per sample. Options: report.numReads","title":"RegisterNumReads"},{"location":"module/implicit/module.implicit.parser/","text":"biolockj.module.implicit.parser modules parse classifier output to generate OTU tables. Implicit modules are ignored if included in the Config file unless project.allowImplicitModules =Y. This package contains the following sub-packages: module.implicit.parser.r16s modules parse module.classifier.r16s reports. module.implicit.parser.wgs modules parse module.classifier.wgs reports. ParserModuleImpl cannot be included in the pipeline run order Description: Abstract implementation of ParserModule that the other modules extend to inherit standard functionality. Abstract modules cannot be added to a pipeline, but the r16s & WGS sub-packages contain modules that inherit standard parser functionality from this class. Options: report.numHits","title":"Module.implicit.parser"},{"location":"module/implicit/module.implicit.parser/#parsermoduleimpl","text":"cannot be included in the pipeline run order Description: Abstract implementation of ParserModule that the other modules extend to inherit standard functionality. Abstract modules cannot be added to a pipeline, but the r16s & WGS sub-packages contain modules that inherit standard parser functionality from this class. Options: report.numHits","title":"ParserModuleImpl"},{"location":"module/implicit/module.implicit.parser.r16s/","text":"biolockj.module.implicit.parser.r16s is a sub package of module.implicit.parser. Package modules extend ParserModuleImpl to generate OTU tables from 16S classifier output. Implicit modules are ignored if included in the Config file unless project.allowImplicitModules =Y. RdpParser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.r16s.RdpParser Description: Build OTU tables from RDP reports. Options: rdp.minThresholdScore QiimeParser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.r16s.QiimeParser Description: Build OTU tables from QIIME summarize_taxa.py otu_table text file reports. Options: none","title":"Module.implicit.parser.r16s"},{"location":"module/implicit/module.implicit.parser.r16s/#rdpparser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.r16s.RdpParser Description: Build OTU tables from RDP reports. Options: rdp.minThresholdScore","title":"RdpParser"},{"location":"module/implicit/module.implicit.parser.r16s/#qiimeparser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.r16s.QiimeParser Description: Build OTU tables from QIIME summarize_taxa.py otu_table text file reports. Options: none","title":"QiimeParser"},{"location":"module/implicit/module.implicit.parser.wgs/","text":"biolockj.module.implicit.parser.wgs is a sub package of module.implicit.parser. Package modules extend ParserModuleImpl to generate OTU tables from WGS classifier output. Implicit modules are ignored if included in the Config file unless project.allowImplicitModules =Y. Humann2Parser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.Humann2Parser Description: Build OTU tables from HumanN2 classifier module output. Options: none KrakenParser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.KrakenParser Description: Build OTU tables from KRAKEN mpa-format reports. Options: none Kraken2Parser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.Kraken2Parser Description: Build OTU tables from KRAKEN 2 mpa-format reports. Options: none Metaphlan2Parser (added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.MetaphlanParser Description: Build OTU tables from Metaphlan2 classifier module reports. Options: none","title":"Module.implicit.parser.wgs"},{"location":"module/implicit/module.implicit.parser.wgs/#humann2parser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.Humann2Parser Description: Build OTU tables from HumanN2 classifier module output. Options: none","title":"Humann2Parser"},{"location":"module/implicit/module.implicit.parser.wgs/#krakenparser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.KrakenParser Description: Build OTU tables from KRAKEN mpa-format reports. Options: none","title":"KrakenParser"},{"location":"module/implicit/module.implicit.parser.wgs/#kraken2parser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.Kraken2Parser Description: Build OTU tables from KRAKEN 2 mpa-format reports. Options: none","title":"Kraken2Parser"},{"location":"module/implicit/module.implicit.parser.wgs/#metaphlan2parser","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.parser.wgs.MetaphlanParser Description: Build OTU tables from Metaphlan2 classifier module reports. Options: none","title":"Metaphlan2Parser"},{"location":"module/implicit/module.implicit.qiime/","text":"biolockj.module.implicit.qiime modules are QIIME Script wrappers implicitly added (if needed). Implicit modules are ignored if included in the Config file unless project.allowImplicitModules =Y. BuildQiimeMapping (added by BioLockJ) #BioModule biolockj.module.implicit.qiime.BuildQiimeMapping Description: This module builds a QIIME mapping file from the metadata. If the metadata file contains the correct columns out of order, awk is used to correct the column order. The updated mapping file is verified with the QIIME script validate_mapping_file.py Options: exe.awk QiimeClassifier (added by BioLockJ) #BioModule biolockj.module.implicit.qiime.QiimeClassifier Description: Generates bash script lines to summarize QIIME results, build taxonomy reports, and add alpha diversity metrics (if configured). For a complete list of available metrics, see: http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html Options: qiime.alphaMetrics qiime.pynastAlignDB qiime.refSeqDB qiime.removeChimeras qiime.taxaDB MergeQiimeOtuTables (added by BioLockJ) #BioModule biolockj.module.implicit.qiime.MergeQiimeOtuTables Description: This module runs the QIIME script merge_otu_tables.py to combine the multiple otu_table.biom files output by its required prerequisite module QiimeClosedRefClassifier , so is only necessary if #samples > script.batchSize . Options: none","title":"Module.implicit.qiime"},{"location":"module/implicit/module.implicit.qiime/#buildqiimemapping","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.qiime.BuildQiimeMapping Description: This module builds a QIIME mapping file from the metadata. If the metadata file contains the correct columns out of order, awk is used to correct the column order. The updated mapping file is verified with the QIIME script validate_mapping_file.py Options: exe.awk","title":"BuildQiimeMapping"},{"location":"module/implicit/module.implicit.qiime/#qiimeclassifier","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.qiime.QiimeClassifier Description: Generates bash script lines to summarize QIIME results, build taxonomy reports, and add alpha diversity metrics (if configured). For a complete list of available metrics, see: http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html Options: qiime.alphaMetrics qiime.pynastAlignDB qiime.refSeqDB qiime.removeChimeras qiime.taxaDB","title":"QiimeClassifier"},{"location":"module/implicit/module.implicit.qiime/#mergeqiimeotutables","text":"(added by BioLockJ) #BioModule biolockj.module.implicit.qiime.MergeQiimeOtuTables Description: This module runs the QIIME script merge_otu_tables.py to combine the multiple otu_table.biom files output by its required prerequisite module QiimeClosedRefClassifier , so is only necessary if #samples > script.batchSize . Options: none","title":"MergeQiimeOtuTables"},{"location":"module/report/module.report.humann2/","text":"Pathway Modules Modules in the biolockj.module.report.humann2 sub-package use ParserModule output to produce and process pathway tables, such as those produced by HumanN2 . Humann2CountModule cannot be included in the pipeline run order Description: Abstract class extends JavaModuleImpl that other humann2 classes extend to inherit shared functionality. Abstract modules cannot be included in the pipeline run order. Options: humann2.disablePathAbundance humann2.disablePathCoverage humann2.disableGeneFamilies AddMetadataToPathwayTables #BioModule biolockj.module.report.humann2.AddMetadataToPathwayTables Description: Add metadata columns to the OTU abundance tables. Options: none RemoveLowPathwayCounts #BioModule biolockj.module.report.humann2.RemoveLowPathwayCounts Description: This BioModule Pathway counts below a configured threshold to zero. These low sample counts are assumed to be miscategorized or genomic contamination. Options: report.minCount RemoveScarcePathwayCounts #BioModule biolockj.module.report.humann2.RemoveScarcePathwayCounts Description: This BioModule removes scarce pathways not found in enough samples. Each pathway must be found in a configurable percentage of samples to be retained. Options: report.scarceCountCutoff","title":"Pathway Modules"},{"location":"module/report/module.report.humann2/#pathway-modules","text":"Modules in the biolockj.module.report.humann2 sub-package use ParserModule output to produce and process pathway tables, such as those produced by HumanN2 .","title":"Pathway Modules"},{"location":"module/report/module.report.humann2/#humann2countmodule","text":"cannot be included in the pipeline run order Description: Abstract class extends JavaModuleImpl that other humann2 classes extend to inherit shared functionality. Abstract modules cannot be included in the pipeline run order. Options: humann2.disablePathAbundance humann2.disablePathCoverage humann2.disableGeneFamilies","title":"Humann2CountModule"},{"location":"module/report/module.report.humann2/#addmetadatatopathwaytables","text":"#BioModule biolockj.module.report.humann2.AddMetadataToPathwayTables Description: Add metadata columns to the OTU abundance tables. Options: none","title":"AddMetadataToPathwayTables"},{"location":"module/report/module.report.humann2/#removelowpathwaycounts","text":"#BioModule biolockj.module.report.humann2.RemoveLowPathwayCounts Description: This BioModule Pathway counts below a configured threshold to zero. These low sample counts are assumed to be miscategorized or genomic contamination. Options: report.minCount","title":"RemoveLowPathwayCounts"},{"location":"module/report/module.report.humann2/#removescarcepathwaycounts","text":"#BioModule biolockj.module.report.humann2.RemoveScarcePathwayCounts Description: This BioModule removes scarce pathways not found in enough samples. Each pathway must be found in a configurable percentage of samples to be retained. Options: report.scarceCountCutoff","title":"RemoveScarcePathwayCounts"},{"location":"module/report/module.report/","text":"Report Package Modules in the biolockj.module.report package process ParserModule output, merge the OTU tables with the metadata, and can generate various reports and notifications. This package contains the following sub-packages: module.report.otu contains modules designed to produce or process otu tables. module.report.taxa contains modules designed to produce or process taxa tables. module.report.r contains modules that use R to generate statistics and/or visualizations. module.report.humann2 contains modules designed to produce or process pathway tables. Email #BioModule biolockj.module.report.Email Description: Notify user pipeline is complete by emailing out the pipeline summary. Options: mail.encryptedPassword mail.from mail.smtp.auth mail.smtp.host mail.smtp.port mail.smtp.starttls.enable mail.to JsonReport #BioModule biolockj.module.report.JsonReport Description: This module builds a JSON file from the ParserModule output. Options: report.logBase report.taxonomyLevels","title":"Report Package"},{"location":"module/report/module.report/#report-package","text":"Modules in the biolockj.module.report package process ParserModule output, merge the OTU tables with the metadata, and can generate various reports and notifications. This package contains the following sub-packages: module.report.otu contains modules designed to produce or process otu tables. module.report.taxa contains modules designed to produce or process taxa tables. module.report.r contains modules that use R to generate statistics and/or visualizations. module.report.humann2 contains modules designed to produce or process pathway tables.","title":"Report Package"},{"location":"module/report/module.report/#email","text":"#BioModule biolockj.module.report.Email Description: Notify user pipeline is complete by emailing out the pipeline summary. Options: mail.encryptedPassword mail.from mail.smtp.auth mail.smtp.host mail.smtp.port mail.smtp.starttls.enable mail.to","title":"Email"},{"location":"module/report/module.report/#jsonreport","text":"#BioModule biolockj.module.report.JsonReport Description: This module builds a JSON file from the ParserModule output. Options: report.logBase report.taxonomyLevels","title":"JsonReport"},{"location":"module/report/module.report.otu/","text":"OTU report modules Modules in the biolockj.module.report sub-pakcage normalize ParserModule output, merge the OTU tables with the metadata, or process OTU tables. CompileOtuCounts #BioModule biolockj.module.report.otu.CompileOtuCounts Description: Compiles the counts from all OTU count files into a single summary OTU count file containing OTU counts for the entire dataset. Options: none RarefyOtuCounts #BioModule biolockj.module.report.otu.RarefyOtuCounts Description: Applies a mean iterative post-OTU classification rarefication algorithm so that each output sample will have approximately the same number of OTUs. Options: rarefyOtuCounts.iterations rarefyOtuCounts.lowAbundantCutoff rarefyOtuCounts.quantile rarefyOtuCounts.removeSamplesBelowQuantile RemoveLowOtuCounts #BioModule biolockj.module.report.otu.RemoveLowOtuCounts Description: Removes OTUs with counts below report.minCount . Options: report.minCount report.numHits RemoveScarceOtuCounts #BioModule biolockj.module.report.otu.RemoveScarceOtuCounts Description: Removes OTUs that are not found in enough samples. Options: report.scarceCountCutoff","title":"OTU report modules"},{"location":"module/report/module.report.otu/#otu-report-modules","text":"Modules in the biolockj.module.report sub-pakcage normalize ParserModule output, merge the OTU tables with the metadata, or process OTU tables.","title":"OTU report modules"},{"location":"module/report/module.report.otu/#compileotucounts","text":"#BioModule biolockj.module.report.otu.CompileOtuCounts Description: Compiles the counts from all OTU count files into a single summary OTU count file containing OTU counts for the entire dataset. Options: none","title":"CompileOtuCounts"},{"location":"module/report/module.report.otu/#rarefyotucounts","text":"#BioModule biolockj.module.report.otu.RarefyOtuCounts Description: Applies a mean iterative post-OTU classification rarefication algorithm so that each output sample will have approximately the same number of OTUs. Options: rarefyOtuCounts.iterations rarefyOtuCounts.lowAbundantCutoff rarefyOtuCounts.quantile rarefyOtuCounts.removeSamplesBelowQuantile","title":"RarefyOtuCounts"},{"location":"module/report/module.report.otu/#removelowotucounts","text":"#BioModule biolockj.module.report.otu.RemoveLowOtuCounts Description: Removes OTUs with counts below report.minCount . Options: report.minCount report.numHits","title":"RemoveLowOtuCounts"},{"location":"module/report/module.report.otu/#removescarceotucounts","text":"#BioModule biolockj.module.report.otu.RemoveScarceOtuCounts Description: Removes OTUs that are not found in enough samples. Options: report.scarceCountCutoff","title":"RemoveScarceOtuCounts"},{"location":"module/report/module.report.r/","text":"R Report Modules Modules in the biolockj.module.report.r sub-package generate the statistical analysis and visualizations by executing R scripts. The statistical analysis is performed on the taxa abundance tables generated by AddMetadataToTaxaTables. R_Module cannot be included in the pipeline run order Description: Abstract implementation of ScriptModule that other R modules extend to inherit standard R script functionality. Abstract modules cannot be included in the pipeline run order. Options: exe.rScript r.debug r.nominalFields r.numericFields r.rareOtuThreshold r.reportFields r.saveRData r.timeout report.numHits report.numReads report.taxonomyLevel R_CalculateStats #BioModule biolockj.module.report.r.R_CalculateStats Description: Generate a summary statistics table with [adjusted and unadjusted] [parameteric and non-parametirc] p-values and r 2 values for each reportable metadata field and each report.taxonomyLevel configured. Options: r_CalculateStats.pAdjustMethod r_CalculateStats.pAdjustScope R_PlotEffectSize #BioModule biolockj.module.report.r.R_PlotEffectSize Description: Generate horizontal barplot representing effect size (Cohen's d, r 2 , and/or fold change) for each reportable metadata field and each report.taxonomyLevel configured. Options: r_PlotEffectSize.parametricPval r_PlotEffectSize.disablePvalAdj r_PlotEffectSize.excludePvalAbove r_PlotEffectSize.taxa r_PlotEffectSize.maxNumTaxa r_PlotEffectSize.disableCohensD r_PlotEffectSize.disableRSquared r_PlotEffectSize.disableFoldChange r.colorHighlight r.pvalCutoff R_PlotMds #BioModule biolockj.module.report.r.R_PlotMds Description: Generate sets of multidimensional scaling plots showing 2 axes at a time (up to the < r_PlotMds.numAxis >th axis) with color coding based on each categorical metadata field (default) or by each field given in r_PlotMds.reportFields . Options: r_PlotMds.numAxis r_PlotMds.reportFields r_PlotMds.distance r.colorPalette r.colorPoint r.pch r.pvalCutoff r.pValFormat R_PlotOtus #BioModule biolockj.module.report.r.R_PlotOtus Description: Generate OTU-metadata box-plots and scatter-plots for each reportable metadata field and each report.taxonomyLevel configured Options: r.colorBase r.colorHighlight r.colorPalette r.colorPoint r.pch r.pvalCutoff r.rareOtuThreshold r.pValFormat R_PlotPvalHistograms #BioModule biolockj.module.report.r.R_PlotPvalHistograms Description: Generate p-value histograms for each reportable metadata field and each report.taxonomyLevel configured Options: r.pvalCutoff","title":"R Report Modules"},{"location":"module/report/module.report.r/#r-report-modules","text":"Modules in the biolockj.module.report.r sub-package generate the statistical analysis and visualizations by executing R scripts. The statistical analysis is performed on the taxa abundance tables generated by AddMetadataToTaxaTables.","title":"R Report Modules"},{"location":"module/report/module.report.r/#r_module","text":"cannot be included in the pipeline run order Description: Abstract implementation of ScriptModule that other R modules extend to inherit standard R script functionality. Abstract modules cannot be included in the pipeline run order. Options: exe.rScript r.debug r.nominalFields r.numericFields r.rareOtuThreshold r.reportFields r.saveRData r.timeout report.numHits report.numReads report.taxonomyLevel","title":"R_Module"},{"location":"module/report/module.report.r/#r_calculatestats","text":"#BioModule biolockj.module.report.r.R_CalculateStats Description: Generate a summary statistics table with [adjusted and unadjusted] [parameteric and non-parametirc] p-values and r 2 values for each reportable metadata field and each report.taxonomyLevel configured. Options: r_CalculateStats.pAdjustMethod r_CalculateStats.pAdjustScope","title":"R_CalculateStats"},{"location":"module/report/module.report.r/#r_ploteffectsize","text":"#BioModule biolockj.module.report.r.R_PlotEffectSize Description: Generate horizontal barplot representing effect size (Cohen's d, r 2 , and/or fold change) for each reportable metadata field and each report.taxonomyLevel configured. Options: r_PlotEffectSize.parametricPval r_PlotEffectSize.disablePvalAdj r_PlotEffectSize.excludePvalAbove r_PlotEffectSize.taxa r_PlotEffectSize.maxNumTaxa r_PlotEffectSize.disableCohensD r_PlotEffectSize.disableRSquared r_PlotEffectSize.disableFoldChange r.colorHighlight r.pvalCutoff","title":"R_PlotEffectSize"},{"location":"module/report/module.report.r/#r_plotmds","text":"#BioModule biolockj.module.report.r.R_PlotMds Description: Generate sets of multidimensional scaling plots showing 2 axes at a time (up to the < r_PlotMds.numAxis >th axis) with color coding based on each categorical metadata field (default) or by each field given in r_PlotMds.reportFields . Options: r_PlotMds.numAxis r_PlotMds.reportFields r_PlotMds.distance r.colorPalette r.colorPoint r.pch r.pvalCutoff r.pValFormat","title":"R_PlotMds"},{"location":"module/report/module.report.r/#r_plototus","text":"#BioModule biolockj.module.report.r.R_PlotOtus Description: Generate OTU-metadata box-plots and scatter-plots for each reportable metadata field and each report.taxonomyLevel configured Options: r.colorBase r.colorHighlight r.colorPalette r.colorPoint r.pch r.pvalCutoff r.rareOtuThreshold r.pValFormat","title":"R_PlotOtus"},{"location":"module/report/module.report.r/#r_plotpvalhistograms","text":"#BioModule biolockj.module.report.r.R_PlotPvalHistograms Description: Generate p-value histograms for each reportable metadata field and each report.taxonomyLevel configured Options: r.pvalCutoff","title":"R_PlotPvalHistograms"},{"location":"module/report/module.report.taxa/","text":"Modules in the biolockj.module.report.taxa package process ParserModule output to produce or process taxa tables. AddMetadataToTaxaTables #BioModule biolockj.module.report.taxa.AddMetadataToTaxaTables Description: Map metadata onto taxa tables using sample ID. Options: metadata.columnDelim metadata.commentChar metadata.filePath metadata.nullValue report.taxonomyLevels BuildTaxaTables #BioModule biolockj.module.report.taxa.BuildTaxaTables Description: Process ParserModule output to produce taxa tables. This module reads the most recent OTU count files generated by any previous BioModule and re-writes the data as separate tables containing taxa counts for each taxonomy level. Options: report.taxonomyLevels LogTransformTaxaTables #BioModule biolockj.module.report.taxa.LogTransformTaxaTables Description: Log-transform the raw taxa counts on Log10 or Log-e scales. Options: report.logBase NormalizeTaxaTables #BioModule biolockj.module.report.taxa.NormalizeTaxaTables Description: Normalize taxa tables Options: report.logBase","title":"Module.report.taxa"},{"location":"module/report/module.report.taxa/#addmetadatatotaxatables","text":"#BioModule biolockj.module.report.taxa.AddMetadataToTaxaTables Description: Map metadata onto taxa tables using sample ID. Options: metadata.columnDelim metadata.commentChar metadata.filePath metadata.nullValue report.taxonomyLevels","title":"AddMetadataToTaxaTables"},{"location":"module/report/module.report.taxa/#buildtaxatables","text":"#BioModule biolockj.module.report.taxa.BuildTaxaTables Description: Process ParserModule output to produce taxa tables. This module reads the most recent OTU count files generated by any previous BioModule and re-writes the data as separate tables containing taxa counts for each taxonomy level. Options: report.taxonomyLevels","title":"BuildTaxaTables"},{"location":"module/report/module.report.taxa/#logtransformtaxatables","text":"#BioModule biolockj.module.report.taxa.LogTransformTaxaTables Description: Log-transform the raw taxa counts on Log10 or Log-e scales. Options: report.logBase","title":"LogTransformTaxaTables"},{"location":"module/report/module.report.taxa/#normalizetaxatables","text":"#BioModule biolockj.module.report.taxa.NormalizeTaxaTables Description: Normalize taxa tables Options: report.logBase","title":"NormalizeTaxaTables"},{"location":"module/seq/module.seq/","text":"Seq Package Modules from the biolockj.module.seq package prepare sequence data or metadata prior to classification. If included, seq modules must be ordered to run before modules from any of the other packages. AwkFastaConverter #BioModule biolockj.module.seq.AwkFastaConverter Description: Convert fastq files into fasta format (required by QIIME ). Options: exe.awk exe.gzip Gunzipper #BioModule biolockj.module.seq.Gunzipper Description: Decompress gzipped files. Options: exe.gzip KneadData #BioModule biolockj.module.seq.KneadData Description: Runs the Biobakery KneadData program to remove contaminated DNA. Options: kneaddata.dbs exe.kneaddata exe.kneaddataParams Multiplexer #BioModule biolockj.module.seq.Multiplexer Description: Multiplex samples into a single file, or two files (one with forward reads, one with reverse reads) if multiplexing paired reads. BioLockJ modules require demultiplexed data, so if included, this must be the last module in the pipeline other than module.report modules. Options: metadata.barcodeColumn metadata.filePath PearMergeReads #BioModule biolockj.module.seq.PearMergeReads Description: Merge paired reads (required for RDP & QIIME ). For more informations, see the online PEAR manual . Options: exe.pear exe.pearParams RarefySeqs #BioModule biolockj.module.seq.RarefySeqs Description: Randomly select samples to reduce all samples to the configured maximum. Samples with less than the minimum number of reads are discarded. Options: rarefySeqs.max rarefySeqs.min SeqFileValidator #BioModule biolockj.module.seq.SeqFileValidator Description: This BioModule validates fasta/fastq file formats are valid and enforces min/max read lengths. Options: input.seqMaxLen input.seqMinLen TrimPrimers #BioModule biolockj.module.seq.TrimPrimers Description: Remove primers from reads, option to discard reads unless primers are attached to both forward and reverse reads. Options: trimPrimers.filePath trimPrimers.requirePrimer","title":"Seq Package"},{"location":"module/seq/module.seq/#seq-package","text":"Modules from the biolockj.module.seq package prepare sequence data or metadata prior to classification. If included, seq modules must be ordered to run before modules from any of the other packages.","title":"Seq Package"},{"location":"module/seq/module.seq/#awkfastaconverter","text":"#BioModule biolockj.module.seq.AwkFastaConverter Description: Convert fastq files into fasta format (required by QIIME ). Options: exe.awk exe.gzip","title":"AwkFastaConverter"},{"location":"module/seq/module.seq/#gunzipper","text":"#BioModule biolockj.module.seq.Gunzipper Description: Decompress gzipped files. Options: exe.gzip","title":"Gunzipper"},{"location":"module/seq/module.seq/#kneaddata","text":"#BioModule biolockj.module.seq.KneadData Description: Runs the Biobakery KneadData program to remove contaminated DNA. Options: kneaddata.dbs exe.kneaddata exe.kneaddataParams","title":"KneadData"},{"location":"module/seq/module.seq/#multiplexer","text":"#BioModule biolockj.module.seq.Multiplexer Description: Multiplex samples into a single file, or two files (one with forward reads, one with reverse reads) if multiplexing paired reads. BioLockJ modules require demultiplexed data, so if included, this must be the last module in the pipeline other than module.report modules. Options: metadata.barcodeColumn metadata.filePath","title":"Multiplexer"},{"location":"module/seq/module.seq/#pearmergereads","text":"#BioModule biolockj.module.seq.PearMergeReads Description: Merge paired reads (required for RDP & QIIME ). For more informations, see the online PEAR manual . Options: exe.pear exe.pearParams","title":"PearMergeReads"},{"location":"module/seq/module.seq/#rarefyseqs","text":"#BioModule biolockj.module.seq.RarefySeqs Description: Randomly select samples to reduce all samples to the configured maximum. Samples with less than the minimum number of reads are discarded. Options: rarefySeqs.max rarefySeqs.min","title":"RarefySeqs"},{"location":"module/seq/module.seq/#seqfilevalidator","text":"#BioModule biolockj.module.seq.SeqFileValidator Description: This BioModule validates fasta/fastq file formats are valid and enforces min/max read lengths. Options: input.seqMaxLen input.seqMinLen","title":"SeqFileValidator"},{"location":"module/seq/module.seq/#trimprimers","text":"#BioModule biolockj.module.seq.TrimPrimers Description: Remove primers from reads, option to discard reads unless primers are attached to both forward and reverse reads. Options: trimPrimers.filePath trimPrimers.requirePrimer","title":"TrimPrimers"}]}